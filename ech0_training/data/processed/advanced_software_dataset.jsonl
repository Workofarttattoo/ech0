{"prompt": "Explain when and why to use different data structures.\n\nInput:\nArray vs Linked List vs Hash Table vs Binary Search Tree - when should I use each?", "completion": "**Data Structure Selection Guide:**\n\n**1. ARRAY**\n\n**When to Use:**\n- Need fast random access by index (O(1))\n- Know size in advance (or can estimate)\n- Sequential iteration is common\n- Memory locality matters (cache-friendly)\n\n**Use Cases:**\n- Fixed-size collections\n- Lookup tables\n- Matrix operations\n- Image pixel data\n- Time-series data\n\n**Operations:**\n- Access: O(1)\n- Search: O(n) unsorted, O(log n) sorted\n- Insert/Delete: O(n) (shifting required)\n- Space: O(n)\n\n**Example:**\n```python\n# Fast random access needed\nscores = [95, 87, 92, 78, 88]\nprint(scores[2])  # O(1) access\n```\n\n---\n\n**2. LINKED LIST**\n\n**When to Use:**\n- Frequent insertions/deletions at beginning\n- Don't know size in advance\n- Don't need random access\n- Implementing stacks/queues\n\n**Use Cases:**\n- Undo functionality (doubly-linked)\n- Music playlist (next/previous)\n- Browser history\n- Memory management (free lists)\n\n**Operations:**\n- Access: O(n)\n- Search: O(n)\n- Insert/Delete at head: O(1)\n- Insert/Delete at arbitrary position: O(n)\n- Space: O(n) + pointer overhead\n\n**Example:**\n```python\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n# Efficient insertion at front\ndef prepend(head, data):\n    new_node = Node(data)\n    new_node.next = head\n    return new_node  # O(1)\n```\n\n---\n\n**3. HASH TABLE (Dictionary/HashMap)**\n\n**When to Use:**\n- Need O(1) average lookups\n- Key-value association needed\n- Checking existence (membership testing)\n- Counting occurrences\n\n**Use Cases:**\n- Caching\n- Database indexing\n- Frequency counting\n- Removing duplicates\n- Implementing sets\n\n**Operations:**\n- Access: O(1) average, O(n) worst case\n- Insert: O(1) average\n- Delete: O(1) average\n- Space: O(n)\n\n**Example:**\n```python\n# Fast lookups and counting\nword_count = {}\nfor word in text:\n    word_count[word] = word_count.get(word, 0) + 1\n\n# O(1) lookup\nif 'python' in word_count:\n    print(word_count['python'])\n```\n\n---\n\n**4. BINARY SEARCH TREE (BST)**\n\n**When to Use:**\n- Need data sorted\n- Range queries needed\n- Need both O(log n) search AND maintain order\n- Predecessor/successor queries\n\n**Use Cases:**\n- Maintaining sorted streaming data\n- Range searches (find all between X and Y)\n- Auto-complete systems\n- Priority queues (with heap variant)\n\n**Operations (balanced BST):**\n- Access: O(log n)\n- Search: O(log n)\n- Insert: O(log n)\n- Delete: O(log n)\n- Traversal in sorted order: O(n)\n- Space: O(n)\n\n**Example:**\n```python\nclass TreeNode:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef search_bst(root, target):\n    if not root or root.val == target:\n        return root\n    if target < root.val:\n        return search_bst(root.left, target)\n    return search_bst(root.right, target)\n\n# O(log n) search in balanced tree\n```\n\n---\n\n**DECISION FLOWCHART:**\n\n```\nStart\n  |\n  ↓\n[Need O(1) lookups by key?]\n  ├─Yes→ HASH TABLE\n  └─No→ [Need sorted order?]\n          ├─Yes→ [Frequent inserts/deletes?]\n          │       ├─Yes→ BINARY SEARCH TREE\n          │       └─No→ SORTED ARRAY + Binary Search\n          └─No→ [Frequent inserts/deletes?]\n                  ├─Yes (at front)→ LINKED LIST\n                  └─No→ [Random access needed?]\n                          ├─Yes→ ARRAY\n                          └─No→ LINKED LIST\n```\n\n**COMPARISON TABLE:**\n\n| Operation | Array | Linked List | Hash Table | BST (balanced) |\n|-----------|-------|-------------|------------|----------------|\n| Access by index | O(1) | O(n) | N/A | N/A |\n| Search | O(n) | O(n) | O(1) avg | O(log n) |\n| Insert (start) | O(n) | O(1) | O(1) avg | O(log n) |\n| Insert (end) | O(1)* | O(n) | O(1) avg | O(log n) |\n| Delete | O(n) | O(1)** | O(1) avg | O(log n) |\n| Sorted traversal | O(n log n) | O(n log n) | N/A | O(n) |\n| Space overhead | Low | High | Medium | Medium |\n| Cache-friendly | Yes | No | No | No |\n\n*Amortized for dynamic arrays\n**If you have pointer to node\n\n**REAL-WORLD EXAMPLES:**\n\n```python\n# Use Array: Simple list of items\nitems = [1, 2, 3, 4, 5]\n\n# Use Hash Table: Fast lookups\nuser_cache = {user_id: user_data}\n\n# Use BST: Sorted data with updates\nfrom sortedcontainers import SortedDict\nleaderboard = SortedDict()  # Maintains score order\n\n# Use Linked List: Undo/Redo\nclass UndoManager:\n    def __init__(self):\n        self.history = LinkedList()  # Easy insert/delete\n```\n\n**KEY TAKEAWAY:**\n- **Array**: Known size, random access, cache-friendly\n- **Hash Table**: Key-value, O(1) lookups, no order\n- **BST**: Sorted order, O(log n) operations, range queries\n- **Linked List**: Dynamic size, frequent insertions at ends\n\nChoose based on your access patterns and performance requirements!", "tags": ["advanced_software_dataset", "algorithms_data_structures"], "quality_score": 0.7, "metadata": {"legacy_domain": "advanced_software", "source_file": null}}
{"prompt": "Design a distributed URL shortener system like bit.ly.\n\nInput:\nHandle 1000 writes/second and 10,000 reads/second. Ensure high availability and low latency.", "completion": "**URL Shortener System Design:**\n\n**Requirements Analysis:**\n- **Functional:**\n  - Generate short URLs from long URLs\n  - Redirect short URLs to original URLs\n  - Custom aliases (optional)\n  - Analytics (click tracking)\n\n- **Non-Functional:**\n  - 1000 writes/sec, 10,000 reads/sec (10:1 read-heavy)\n  - Low latency (<100ms)\n  - High availability (99.9%)\n  - Scalable\n\n**Capacity Estimation:**\n```\nWrites: 1000/sec\n= 86.4M/day\n= 31.5B/year\n= 315B over 10 years\n\nStorage per URL: ~500 bytes (URL + metadata)\nTotal storage: 315B × 500B ≈ 157.5 TB\n\nReads: 10,000/sec = 864M/day\n```\n\n**High-Level Architecture:**\n```\n┌─────────┐\n│ Clients │\n└────┬────┘\n     │\n     ↓\n┌──────────────┐\n│ Load Balancer│ (NGINX/AWS ELB)\n└──────┬───────┘\n       │\n       ├─────────────────────┬──────────────────┐\n       ↓                     ↓                  ↓\n ┌──────────┐          ┌──────────┐      ┌──────────┐\n │ API      │          │ API      │      │ API      │\n │ Servers  │          │ Servers  │      │ Servers  │\n └─────┬────┘          └─────┬────┘      └─────┬────┘\n       │                     │                  │\n       ├─────────────────────┴──────────────────┤\n       │                                        │\n       ↓                                        ↓\n┌──────────────┐                         ┌──────────┐\n│ Cache Layer  │                         │ Database │\n│  (Redis)     │                         │ Cluster  │\n└──────────────┘                         └──────────┘\n       │\n       ↓\n┌──────────────┐\n│ Analytics DB │\n│ (ClickHouse) │\n└──────────────┘\n```\n\n**Key Design Decisions:**\n\n**1. URL Encoding Strategy:**\n\nOption A: Hash-based (MD5/SHA256)\n- Pros: Fast, deterministic\n- Cons: Collision handling needed, not shortest\n\nOption B: Base62 Encoding of Auto-increment ID ✅\n```python\ndef encode_base62(num: int) -> str:\n    chars = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    if num == 0:\n        return chars[0]\n    result = []\n    while num:\n        result.append(chars[num % 62])\n        num //= 62\n    return ''.join(reversed(result))\n\n# Example:\n# ID 1 → '1'\n# ID 62 → '10'\n# ID 1000000 → '4c92'\n```\n\n**Benefits:**\n- 6 characters = 62^6 = 56.8 billion combinations\n- 7 characters = 62^7 = 3.5 trillion combinations\n- No collisions (unique IDs)\n- Short URLs\n\n**2. Database Design:**\n\n**Schema:**\n```sql\nCREATE TABLE urls (\n    id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    short_code VARCHAR(10) UNIQUE NOT NULL,\n    long_url VARCHAR(2048) NOT NULL,\n    user_id BIGINT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    expires_at TIMESTAMP NULL,\n    INDEX idx_short_code (short_code),\n    INDEX idx_user_id (user_id)\n);\n\nCREATE TABLE clicks (\n    id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    short_code VARCHAR(10),\n    clicked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    user_agent VARCHAR(255),\n    ip_address VARCHAR(45),\n    referer VARCHAR(255),\n    INDEX idx_short_code (short_code),\n    INDEX idx_clicked_at (clicked_at)\n);\n```\n\n**3. Caching Strategy:**\n\n```\nRedis Cache (TTL: 24 hours)\n├── Key: short_code\n├── Value: {long_url, user_id, created_at}\n└── Eviction: LRU\n\nCache Hit Rate: ~90% (read-heavy workload)\n```\n\n**Read Flow:**\n```\n1. Check Redis cache\n   ├── Hit → Return long_url (1-5ms)\n   └── Miss → Query database → Update cache (20-50ms)\n2. Redirect client (HTTP 301/302)\n3. Async: Log click to analytics DB\n```\n\n**4. Scalability:**\n\n**Database Sharding:**\n```\nShard key: hash(short_code) % num_shards\n\nFor 1000 writes/sec:\n- Start with 4 shards (250 writes/sec each)\n- Vertical partitioning: separate clicks table\n```\n\n**ID Generation:**\nUse distributed ID generator:\n- **Snowflake algorithm** (Twitter's approach)\n- **Database auto-increment with ranges** (Flickr's approach)\n- **UUID** (not ideal - too long)\n\n**5. Availability & Reliability:**\n\n- **Database**: Master-slave replication + failover\n- **Cache**: Redis Cluster with replication\n- **Load Balancer**: Active-passive pair\n- **Multi-region deployment** for geo-distribution\n\n**6. API Design:**\n\n```python\n# Create short URL\nPOST /api/shorten\nBody: {\"long_url\": \"https://example.com/very/long/url\"}\nResponse: {\"short_url\": \"https://short.ly/abc123\"}\n\n# Redirect\nGET /{short_code}\nResponse: HTTP 301 Redirect to long_url\n\n# Analytics\nGET /api/analytics/{short_code}\nResponse: {\"clicks\": 1523, \"countries\": {...}}\n```\n\n**7. Optimizations:**\n\n- **CDN** for static content\n- **Connection pooling** for databases\n- **Bloom filter** for non-existent URLs (avoid DB queries)\n- **Rate limiting** to prevent abuse\n- **Async analytics** to not slow down redirects\n\n**8. Monitoring:**\n\n- Latency (p50, p95, p99)\n- Cache hit rate\n- Database query time\n- Error rates\n- Traffic patterns\n\n**Estimated Costs (AWS):**\n```\n- EC2 (10 × t3.medium): $300/month\n- RDS (db.m5.xlarge): $400/month\n- ElastiCache (cache.m5.large): $150/month\n- ALB: $50/month\n- Data transfer: $100/month\n\nTotal: ~$1000/month for 1M requests/day\n```\n\n**Conclusion:**\nThis design handles the required scale with room for growth, optimizes for the read-heavy workload through aggressive caching, and ensures high availability through redundancy and replication.", "tags": ["advanced_software_dataset", "system_design"], "quality_score": 0.7, "metadata": {"legacy_domain": "advanced_software", "source_file": null}}
