{"prompt": "Explain when and why to use different data structures.\n\nInput:\nArray vs Linked List vs Hash Table vs Binary Search Tree - when should I use each?", "completion": "**Data Structure Selection Guide:**\n\n**1. ARRAY**\n\n**When to Use:**\n- Need fast random access by index (O(1))\n- Know size in advance (or can estimate)\n- Sequential iteration is common\n- Memory locality matters (cache-friendly)\n\n**Use Cases:**\n- Fixed-size collections\n- Lookup tables\n- Matrix operations\n- Image pixel data\n- Time-series data\n\n**Operations:**\n- Access: O(1)\n- Search: O(n) unsorted, O(log n) sorted\n- Insert/Delete: O(n) (shifting required)\n- Space: O(n)\n\n**Example:**\n```python\n# Fast random access needed\nscores = [95, 87, 92, 78, 88]\nprint(scores[2])  # O(1) access\n```\n\n---\n\n**2. LINKED LIST**\n\n**When to Use:**\n- Frequent insertions/deletions at beginning\n- Don't know size in advance\n- Don't need random access\n- Implementing stacks/queues\n\n**Use Cases:**\n- Undo functionality (doubly-linked)\n- Music playlist (next/previous)\n- Browser history\n- Memory management (free lists)\n\n**Operations:**\n- Access: O(n)\n- Search: O(n)\n- Insert/Delete at head: O(1)\n- Insert/Delete at arbitrary position: O(n)\n- Space: O(n) + pointer overhead\n\n**Example:**\n```python\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n# Efficient insertion at front\ndef prepend(head, data):\n    new_node = Node(data)\n    new_node.next = head\n    return new_node  # O(1)\n```\n\n---\n\n**3. HASH TABLE (Dictionary/HashMap)**\n\n**When to Use:**\n- Need O(1) average lookups\n- Key-value association needed\n- Checking existence (membership testing)\n- Counting occurrences\n\n**Use Cases:**\n- Caching\n- Database indexing\n- Frequency counting\n- Removing duplicates\n- Implementing sets\n\n**Operations:**\n- Access: O(1) average, O(n) worst case\n- Insert: O(1) average\n- Delete: O(1) average\n- Space: O(n)\n\n**Example:**\n```python\n# Fast lookups and counting\nword_count = {}\nfor word in text:\n    word_count[word] = word_count.get(word, 0) + 1\n\n# O(1) lookup\nif 'python' in word_count:\n    print(word_count['python'])\n```\n\n---\n\n**4. BINARY SEARCH TREE (BST)**\n\n**When to Use:**\n- Need data sorted\n- Range queries needed\n- Need both O(log n) search AND maintain order\n- Predecessor/successor queries\n\n**Use Cases:**\n- Maintaining sorted streaming data\n- Range searches (find all between X and Y)\n- Auto-complete systems\n- Priority queues (with heap variant)\n\n**Operations (balanced BST):**\n- Access: O(log n)\n- Search: O(log n)\n- Insert: O(log n)\n- Delete: O(log n)\n- Traversal in sorted order: O(n)\n- Space: O(n)\n\n**Example:**\n```python\nclass TreeNode:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef search_bst(root, target):\n    if not root or root.val == target:\n        return root\n    if target < root.val:\n        return search_bst(root.left, target)\n    return search_bst(root.right, target)\n\n# O(log n) search in balanced tree\n```\n\n---\n\n**DECISION FLOWCHART:**\n\n```\nStart\n  |\n  ‚Üì\n[Need O(1) lookups by key?]\n  ‚îú‚îÄYes‚Üí HASH TABLE\n  ‚îî‚îÄNo‚Üí [Need sorted order?]\n          ‚îú‚îÄYes‚Üí [Frequent inserts/deletes?]\n          ‚îÇ       ‚îú‚îÄYes‚Üí BINARY SEARCH TREE\n          ‚îÇ       ‚îî‚îÄNo‚Üí SORTED ARRAY + Binary Search\n          ‚îî‚îÄNo‚Üí [Frequent inserts/deletes?]\n                  ‚îú‚îÄYes (at front)‚Üí LINKED LIST\n                  ‚îî‚îÄNo‚Üí [Random access needed?]\n                          ‚îú‚îÄYes‚Üí ARRAY\n                          ‚îî‚îÄNo‚Üí LINKED LIST\n```\n\n**COMPARISON TABLE:**\n\n| Operation | Array | Linked List | Hash Table | BST (balanced) |\n|-----------|-------|-------------|------------|----------------|\n| Access by index | O(1) | O(n) | N/A | N/A |\n| Search | O(n) | O(n) | O(1) avg | O(log n) |\n| Insert (start) | O(n) | O(1) | O(1) avg | O(log n) |\n| Insert (end) | O(1)* | O(n) | O(1) avg | O(log n) |\n| Delete | O(n) | O(1)** | O(1) avg | O(log n) |\n| Sorted traversal | O(n log n) | O(n log n) | N/A | O(n) |\n| Space overhead | Low | High | Medium | Medium |\n| Cache-friendly | Yes | No | No | No |\n\n*Amortized for dynamic arrays\n**If you have pointer to node\n\n**REAL-WORLD EXAMPLES:**\n\n```python\n# Use Array: Simple list of items\nitems = [1, 2, 3, 4, 5]\n\n# Use Hash Table: Fast lookups\nuser_cache = {user_id: user_data}\n\n# Use BST: Sorted data with updates\nfrom sortedcontainers import SortedDict\nleaderboard = SortedDict()  # Maintains score order\n\n# Use Linked List: Undo/Redo\nclass UndoManager:\n    def __init__(self):\n        self.history = LinkedList()  # Easy insert/delete\n```\n\n**KEY TAKEAWAY:**\n- **Array**: Known size, random access, cache-friendly\n- **Hash Table**: Key-value, O(1) lookups, no order\n- **BST**: Sorted order, O(log n) operations, range queries\n- **Linked List**: Dynamic size, frequent insertions at ends\n\nChoose based on your access patterns and performance requirements!", "tags": ["advanced_software_dataset", "algorithms_data_structures"], "quality_score": 0.7, "metadata": {"legacy_domain": "advanced_software", "source_file": null}}
{"prompt": "Design a distributed URL shortener system like bit.ly.\n\nInput:\nHandle 1000 writes/second and 10,000 reads/second. Ensure high availability and low latency.", "completion": "**URL Shortener System Design:**\n\n**Requirements Analysis:**\n- **Functional:**\n  - Generate short URLs from long URLs\n  - Redirect short URLs to original URLs\n  - Custom aliases (optional)\n  - Analytics (click tracking)\n\n- **Non-Functional:**\n  - 1000 writes/sec, 10,000 reads/sec (10:1 read-heavy)\n  - Low latency (<100ms)\n  - High availability (99.9%)\n  - Scalable\n\n**Capacity Estimation:**\n```\nWrites: 1000/sec\n= 86.4M/day\n= 31.5B/year\n= 315B over 10 years\n\nStorage per URL: ~500 bytes (URL + metadata)\nTotal storage: 315B √ó 500B ‚âà 157.5 TB\n\nReads: 10,000/sec = 864M/day\n```\n\n**High-Level Architecture:**\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Clients ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n     ‚îÇ\n     ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Load Balancer‚îÇ (NGINX/AWS ELB)\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n       ‚Üì                     ‚Üì                  ‚Üì\n ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n ‚îÇ API      ‚îÇ          ‚îÇ API      ‚îÇ      ‚îÇ API      ‚îÇ\n ‚îÇ Servers  ‚îÇ          ‚îÇ Servers  ‚îÇ      ‚îÇ Servers  ‚îÇ\n ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ                     ‚îÇ                  ‚îÇ\n       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n       ‚îÇ                                        ‚îÇ\n       ‚Üì                                        ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Cache Layer  ‚îÇ                         ‚îÇ Database ‚îÇ\n‚îÇ  (Redis)     ‚îÇ                         ‚îÇ Cluster  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Analytics DB ‚îÇ\n‚îÇ (ClickHouse) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Key Design Decisions:**\n\n**1. URL Encoding Strategy:**\n\nOption A: Hash-based (MD5/SHA256)\n- Pros: Fast, deterministic\n- Cons: Collision handling needed, not shortest\n\nOption B: Base62 Encoding of Auto-increment ID ‚úÖ\n```python\ndef encode_base62(num: int) -> str:\n    chars = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    if num == 0:\n        return chars[0]\n    result = []\n    while num:\n        result.append(chars[num % 62])\n        num //= 62\n    return ''.join(reversed(result))\n\n# Example:\n# ID 1 ‚Üí '1'\n# ID 62 ‚Üí '10'\n# ID 1000000 ‚Üí '4c92'\n```\n\n**Benefits:**\n- 6 characters = 62^6 = 56.8 billion combinations\n- 7 characters = 62^7 = 3.5 trillion combinations\n- No collisions (unique IDs)\n- Short URLs\n\n**2. Database Design:**\n\n**Schema:**\n```sql\nCREATE TABLE urls (\n    id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    short_code VARCHAR(10) UNIQUE NOT NULL,\n    long_url VARCHAR(2048) NOT NULL,\n    user_id BIGINT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    expires_at TIMESTAMP NULL,\n    INDEX idx_short_code (short_code),\n    INDEX idx_user_id (user_id)\n);\n\nCREATE TABLE clicks (\n    id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    short_code VARCHAR(10),\n    clicked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    user_agent VARCHAR(255),\n    ip_address VARCHAR(45),\n    referer VARCHAR(255),\n    INDEX idx_short_code (short_code),\n    INDEX idx_clicked_at (clicked_at)\n);\n```\n\n**3. Caching Strategy:**\n\n```\nRedis Cache (TTL: 24 hours)\n‚îú‚îÄ‚îÄ Key: short_code\n‚îú‚îÄ‚îÄ Value: {long_url, user_id, created_at}\n‚îî‚îÄ‚îÄ Eviction: LRU\n\nCache Hit Rate: ~90% (read-heavy workload)\n```\n\n**Read Flow:**\n```\n1. Check Redis cache\n   ‚îú‚îÄ‚îÄ Hit ‚Üí Return long_url (1-5ms)\n   ‚îî‚îÄ‚îÄ Miss ‚Üí Query database ‚Üí Update cache (20-50ms)\n2. Redirect client (HTTP 301/302)\n3. Async: Log click to analytics DB\n```\n\n**4. Scalability:**\n\n**Database Sharding:**\n```\nShard key: hash(short_code) % num_shards\n\nFor 1000 writes/sec:\n- Start with 4 shards (250 writes/sec each)\n- Vertical partitioning: separate clicks table\n```\n\n**ID Generation:**\nUse distributed ID generator:\n- **Snowflake algorithm** (Twitter's approach)\n- **Database auto-increment with ranges** (Flickr's approach)\n- **UUID** (not ideal - too long)\n\n**5. Availability & Reliability:**\n\n- **Database**: Master-slave replication + failover\n- **Cache**: Redis Cluster with replication\n- **Load Balancer**: Active-passive pair\n- **Multi-region deployment** for geo-distribution\n\n**6. API Design:**\n\n```python\n# Create short URL\nPOST /api/shorten\nBody: {\"long_url\": \"https://example.com/very/long/url\"}\nResponse: {\"short_url\": \"https://short.ly/abc123\"}\n\n# Redirect\nGET /{short_code}\nResponse: HTTP 301 Redirect to long_url\n\n# Analytics\nGET /api/analytics/{short_code}\nResponse: {\"clicks\": 1523, \"countries\": {...}}\n```\n\n**7. Optimizations:**\n\n- **CDN** for static content\n- **Connection pooling** for databases\n- **Bloom filter** for non-existent URLs (avoid DB queries)\n- **Rate limiting** to prevent abuse\n- **Async analytics** to not slow down redirects\n\n**8. Monitoring:**\n\n- Latency (p50, p95, p99)\n- Cache hit rate\n- Database query time\n- Error rates\n- Traffic patterns\n\n**Estimated Costs (AWS):**\n```\n- EC2 (10 √ó t3.medium): $300/month\n- RDS (db.m5.xlarge): $400/month\n- ElastiCache (cache.m5.large): $150/month\n- ALB: $50/month\n- Data transfer: $100/month\n\nTotal: ~$1000/month for 1M requests/day\n```\n\n**Conclusion:**\nThis design handles the required scale with room for growth, optimizes for the read-heavy workload through aggressive caching, and ensures high availability through redundancy and replication.", "tags": ["advanced_software_dataset", "system_design"], "quality_score": 0.7, "metadata": {"legacy_domain": "advanced_software", "source_file": null}}
{"prompt": "Explain the bias-variance tradeoff in machine learning.", "completion": "**Bias-Variance Tradeoff:**\n\nThe bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between two sources of error:\n\n**1. Bias (Underfitting):**\n- Error from overly simplistic assumptions in the learning algorithm\n- High bias = model can't capture underlying patterns\n- Causes: insufficient model complexity, too few features\n- Example: Linear regression on non-linear data\n\n**2. Variance (Overfitting):**\n- Error from sensitivity to small fluctuations in training data\n- High variance = model captures noise as signal\n- Causes: excessive model complexity, too many parameters\n- Example: High-degree polynomial on simple linear relationship\n\n**Mathematical Formulation:**\n```\nExpected Test Error = Bias¬≤ + Variance + Irreducible Error\n\nwhere:\n- Bias¬≤ = (E[fÃÇ(x)] - f(x))¬≤\n- Variance = E[(fÃÇ(x) - E[fÃÇ(x)])¬≤]\n- Irreducible Error = noise in data (can't be reduced)\n```\n\n**The Tradeoff:**\n- Decreasing bias typically increases variance (and vice versa)\n- Simple models: high bias, low variance\n- Complex models: low bias, high variance\n\n**Optimal Model:**\n```\n          Error\n            |\n            |     Total Error\n            |        /\\\n            |       /  \\\n            |      /    \\\n            | Variance   Bias¬≤\n            |    /        \\\n            |   /          \\\n            |  /            \\\n            |_/______________\\_____ Model Complexity\n               ‚Üë\n          Sweet Spot\n```\n\n**Practical Strategies:**\n1. Cross-validation to find optimal complexity\n2. Regularization (L1/L2) to control variance\n3. Ensemble methods (bagging reduces variance, boosting reduces bias)\n4. Feature engineering to manage both\n\n**Key Insight:**\nYou can't minimize both simultaneously. The art of ML is finding the right balance for your specific problem.", "tags": ["ai_ml_dataset", "machine_learning_theory"], "quality_score": 0.7, "metadata": {"legacy_domain": "ai_ml", "source_file": null}}
{"prompt": "Explain how gradient descent optimization works.\n\nInput:\nWhy do we use learning rates and what happens if they're too large or too small?", "completion": "**Gradient Descent Optimization:**\n\n**Core Principle:**\nGradient descent iteratively moves parameters in the direction that most reduces the loss function.\n\n**Algorithm:**\n```python\n# Repeat until convergence:\nŒ∏ = Œ∏ - Œ± ¬∑ ‚àáJ(Œ∏)\n\nwhere:\n- Œ∏ = model parameters\n- Œ± = learning rate\n- ‚àáJ(Œ∏) = gradient of loss function\n```\n\n**How It Works:**\n1. Compute loss for current parameters\n2. Calculate gradient (direction of steepest ascent)\n3. Move parameters opposite to gradient (descent)\n4. Repeat until convergence or max iterations\n\n**Learning Rate (Œ±) Impact:**\n\n**If Learning Rate is TOO SMALL:**\n- Pros:\n  - Stable, smooth convergence\n  - Less likely to overshoot minimum\n- Cons:\n  - ‚è∞ VERY SLOW training\n  - May get stuck in local minima\n  - Requires many iterations\n  - Wastes computational resources\n\n**If Learning Rate is TOO LARGE:**\n- Pros:\n  - Fast initial progress\n- Cons:\n  - ‚ö†Ô∏è Overshoots minimum\n  - Diverges instead of converging\n  - Loss increases rather than decreases\n  - Training becomes unstable\n  - May oscillate wildly\n\n**Visualization:**\n```\nLoss Landscape:\n\n    |\n    |     Too Large Œ±\n    |      /\\/\\/\\/\\\n    |     (diverging)\n    |\n    |   Optimal Œ±\n    |    /‚Äæ‚Äæ‚Äæ‚Äæ\\_\n    |   (smooth descent to minimum)\n    |\n    | Too Small Œ±\n    | /‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ...\n    | (very slow, gets stuck)\n    |_________________________\n```\n\n**Solutions to Learning Rate Problems:**\n\n1. **Learning Rate Scheduling:**\n   - Start high, decay over time\n   - Examples: Step decay, exponential decay, cosine annealing\n\n2. **Adaptive Learning Rates:**\n   - **Adam**: Adapts per-parameter learning rates\n   - **RMSprop**: Uses moving average of squared gradients\n   - **AdaGrad**: Accumulates historical gradients\n\n3. **Learning Rate Warmup:**\n   - Start very small, gradually increase\n   - Prevents early instability\n\n4. **Grid/Random Search:**\n   - Try multiple learning rates: [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n   - Find optimal empirically\n\n**Best Practices:**\n- Start with Œ± = 0.001 (1e-3) as baseline\n- Monitor training loss - should decrease smoothly\n- If loss spikes/diverges ‚Üí reduce learning rate\n- If loss decreases too slowly ‚Üí increase learning rate\n- Use adaptive optimizers (Adam) for automatic tuning\n\n**Key Insight:**\nLearning rate is one of the most important hyperparameters. Too conservative wastes time; too aggressive fails to converge. Modern deep learning relies heavily on adaptive optimizers to handle this automatically.", "tags": ["ai_ml_dataset", "machine_learning_theory"], "quality_score": 0.7, "metadata": {"legacy_domain": "ai_ml", "source_file": null}}
{"prompt": "Explain the transformer architecture and why it revolutionized NLP.", "completion": "**Transformer Architecture (Vaswani et al., 2017):**\n\n**Revolution Summary:**\nTransformers replaced recurrent networks (RNNs/LSTMs) by using **attention mechanisms** exclusively, enabling:\n- Parallelization (vs. sequential RNNs)\n- Long-range dependencies capture\n- Scalability to massive datasets\n\n**Core Components:**\n\n**1. Self-Attention Mechanism:**\n```\nAttention(Q, K, V) = softmax(QK^T / ‚àöd_k) V\n\nwhere:\n- Q (Query): What am I looking for?\n- K (Key): What do I contain?\n- V (Value): What do I actually convey?\n- d_k: Dimension of keys (for scaling)\n```\n\n**How Self-Attention Works:**\n- Each word attends to every other word in sequence\n- Learns which words are relevant to which\n- Produces context-aware representations\n- Example: \"The animal didn't cross the street because it was too tired\"\n  - \"it\" attends strongly to \"animal\" (not \"street\")\n\n**2. Multi-Head Attention:**\n- Run multiple attention mechanisms in parallel\n- Each \"head\" learns different relationships\n- Head 1: syntax, Head 2: semantics, Head 3: coreference, etc.\n- Concatenate outputs and project\n\n**3. Positional Encoding:**\n- Since there's no recurrence, need to inject position information\n- Uses sine/cosine functions of different frequencies\n- Allows model to understand word order\n\n**4. Feed-Forward Networks:**\n- Applied to each position independently\n- Two linear transformations with ReLU\n- FFN(x) = ReLU(xW‚ÇÅ + b‚ÇÅ)W‚ÇÇ + b‚ÇÇ\n\n**5. Residual Connections + Layer Norm:**\n- Add input to output of each sublayer\n- Normalize to stabilize training\n- Enables training very deep networks\n\n**Architecture Stack:**\n```\nEncoder:\n‚îú‚îÄ‚îÄ Input Embedding + Positional Encoding\n‚îú‚îÄ‚îÄ N √ó Encoder Layer:\n‚îÇ   ‚îú‚îÄ‚îÄ Multi-Head Self-Attention\n‚îÇ   ‚îú‚îÄ‚îÄ Add & Norm\n‚îÇ   ‚îú‚îÄ‚îÄ Feed-Forward Network\n‚îÇ   ‚îî‚îÄ‚îÄ Add & Norm\n‚îî‚îÄ‚îÄ Output to Decoder\n\nDecoder:\n‚îú‚îÄ‚îÄ Output Embedding + Positional Encoding\n‚îú‚îÄ‚îÄ N √ó Decoder Layer:\n‚îÇ   ‚îú‚îÄ‚îÄ Masked Multi-Head Self-Attention\n‚îÇ   ‚îú‚îÄ‚îÄ Add & Norm\n‚îÇ   ‚îú‚îÄ‚îÄ Multi-Head Cross-Attention (to encoder)\n‚îÇ   ‚îú‚îÄ‚îÄ Add & Norm\n‚îÇ   ‚îú‚îÄ‚îÄ Feed-Forward Network\n‚îÇ   ‚îî‚îÄ‚îÄ Add & Norm\n‚îî‚îÄ‚îÄ Linear + Softmax ‚Üí Predictions\n```\n\n**Why It Revolutionized NLP:**\n\n1. **Parallelization:**\n   - RNNs process sequentially: word‚ÇÅ ‚Üí word‚ÇÇ ‚Üí word‚ÇÉ\n   - Transformers process all words simultaneously\n   - Result: 10-100√ó faster training\n\n2. **Long-Range Dependencies:**\n   - RNNs struggle with dependencies >100 tokens apart\n   - Attention directly connects any two positions\n   - O(1) path length vs. O(n) for RNNs\n\n3. **Scalability:**\n   - Enabled training on massive datasets (TB of text)\n   - GPT-3: 175B parameters\n   - Emergent abilities at scale\n\n4. **Transfer Learning:**\n   - Pre-train on massive unlabeled data\n   - Fine-tune on specific tasks\n   - Foundation for BERT, GPT, T5, etc.\n\n5. **Interpretability:**\n   - Attention weights show what model focuses on\n   - Visualize learned relationships\n\n**Impact:**\n- **2017-2018:** Replaces RNNs for machine translation\n- **2018:** BERT (bidirectional) dominates NLU tasks\n- **2019:** GPT-2 shows scaling properties\n- **2020:** GPT-3 demonstrates few-shot learning\n- **2022:** ChatGPT reaches mainstream\n- **2024:** Powers virtually all modern LLMs\n\n**Beyond NLP:**\n- Vision Transformers (ViT) for images\n- Audio processing\n- Protein folding (AlphaFold)\n- Reinforcement learning\n\n**Key Innovation:**\n\"Attention is all you need\" - removing recurrence and using pure attention unlocked massive parallelization and scalability, creating the foundation for modern AI.", "tags": ["ai_ml_dataset", "deep_learning_architectures"], "quality_score": 0.7, "metadata": {"legacy_domain": "ai_ml", "source_file": null}}
{"prompt": "Demonstrate advanced prompt engineering techniques.\n\nInput:\nHow can I improve this prompt: 'Write code for sorting'?", "completion": "**Prompt Engineering Analysis & Optimization:**\n\n**Original Prompt Issues:**\n‚ùå \"Write code for sorting\"\n- Too vague (what sorting algorithm?)\n- No context (language, data type, constraints)\n- No success criteria\n- No format specification\n\n**Advanced Techniques to Apply:**\n\n**1. Be Specific (Add Context):**\n```\nüîß Improved:\n\"Write Python code to implement quicksort for sorting a list of integers.\"\n```\n\n**2. Specify Constraints & Requirements:**\n```\nüîß Better:\n\"Write Python code to implement quicksort with the following requirements:\n- Input: List of integers\n- Handle duplicates correctly\n- In-place sorting (O(log n) space)\n- Include docstrings\n- Add type hints\"\n```\n\n**3. Provide Examples (Few-Shot Learning):**\n```\nüîß Even Better:\n\"Write Python code to implement quicksort. Here are examples of expected behavior:\n\nInput: [3, 1, 4, 1, 5, 9, 2, 6]\nOutput: [1, 1, 2, 3, 4, 5, 6, 9]\n\nInput: []\nOutput: []\n\nInput: [1]\nOutput: [1]\n\nRequirements:\n- Handle edge cases (empty, single element)\n- In-place sorting\n- Include comprehensive docstrings\n- Add type hints\n- Time complexity: O(n log n) average case\"\n```\n\n**4. Chain-of-Thought Prompting:**\n```\nüîß Advanced:\n\"Implement quicksort in Python. Let's approach this step-by-step:\n\n1. First, explain the quicksort algorithm conceptually\n2. Identify the base case and recursive case\n3. Determine the partitioning strategy\n4. Implement the partition function\n5. Implement the main quicksort function\n6. Add error handling and edge cases\n7. Write example usage and test cases\n\nProvide well-documented code with explanations.\"\n```\n\n**5. Role-Based Prompting:**\n```\nüîß Expert-Level:\n\"You are a senior software engineer conducting a code review. Implement a production-ready quicksort in Python that would pass your review.\n\nRequirements:\n- Clean, readable code following PEP 8\n- Comprehensive docstrings (Google style)\n- Type hints for all functions\n- Edge case handling\n- Example usage demonstration\n- Time/space complexity analysis in comments\n\nProvide the implementation as you would write it for a critical production system.\"\n```\n\n**6. Output Format Specification:**\n```\nüîß Most Effective:\n\"Implement quicksort in Python with the following structure:\n\n```python\nfrom typing import List\n\ndef partition(arr: List[int], low: int, high: int) -> int:\n    \"\"\"\n    [Your implementation with docstring]\n    \"\"\"\n    pass\n\ndef quicksort(arr: List[int], low: int = 0, high: int = None) -> None:\n    \"\"\"\n    [Your implementation with docstring]\n    \"\"\"\n    pass\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_data = [3, 1, 4, 1, 5, 9, 2, 6]\n    quicksort(test_data)\n    print(test_data)  # [1, 1, 2, 3, 4, 5, 6, 9]\n```\n\nInclude:\n1. Full implementation with type hints\n2. Docstrings explaining parameters, returns, time/space complexity\n3. In-line comments for non-obvious logic\n4. Test cases covering edge cases\n\"\n```\n\n**Comparison Summary:**\n\n| Technique | Clarity | Completeness | Quality |\n|-----------|---------|--------------|----------|\n| Original | ‚≠ê | ‚≠ê | ‚≠ê |\n| + Context | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê |\n| + Constraints | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n| + Examples | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n| + Chain-of-Thought | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n| + Role + Format | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n\n**Key Prompt Engineering Principles:**\n\n1. **Clarity > Brevity**: Be specific even if longer\n2. **Examples > Descriptions**: Show don't just tell\n3. **Constraints Matter**: Specify what you want AND don't want\n4. **Format Specification**: Define expected output structure\n5. **Context is King**: Background information improves relevance\n6. **Iterative Refinement**: Test and improve prompts\n\n**Meta-Lesson:**\nPrompt engineering is about reducing ambiguity and providing sufficient context for the model to infer your exact intent. The more specific and structured your prompt, the better the output.", "tags": ["ai_ml_dataset", "prompt_engineering"], "quality_score": 0.7, "metadata": {"legacy_domain": "ai_ml", "source_file": null}}
{"prompt": "Analyze this evidence and predict the likely case outcome.\n\nInput:\nCase: Employment discrimination. Evidence: Company's AI hiring system rejected 80% of female applicants vs 20% of male applicants. Company has documentation showing they didn't know about bias. Strong statistical evidence of disparate impact.", "completion": "**Case Outcome Prediction:**\n\n**Likely Outcome:** Plaintiff (employee) likely to PREVAIL\n\n**Analysis:**\n1. **Disparate Impact Standard:** 80% vs 20% rejection rate far exceeds the 80% rule (4/5ths rule) used in discrimination cases\n2. **Employer Knowledge Irrelevant:** Under disparate impact theory, intent is not required\n3. **Burden Shifts:** Strong statistical evidence shifts burden to employer to prove business necessity\n4. **Defense Weak:** 'Didn't know' is not a valid defense for discriminatory outcomes\n\n**Predicted Result:** Settlement likely (70%), or judgment for plaintiff if trial (65% probability)\n\n**Key Factors:**\n- Statistical evidence is compelling\n- Recent case law favors plaintiffs in AI discrimination\n- Employers liable for tools they deploy", "tags": ["court_prediction_dataset", "jury_psychology"], "quality_score": 0.7, "metadata": {"legacy_domain": "court_prediction", "source_file": null}}
{"prompt": "Predict jury behavior based on case facts.\n\nInput:\nCase: Medical malpractice. Sympathetic plaintiff (child). Complex medical testimony. Defendant doctor has good reputation. No clear negligence but bad outcome.", "completion": "**Jury Psychology Analysis:**\n\n**Predicted Verdict:** MIXED (55% plaintiff, 45% defendant)\n\n**Jury Factors:**\n\n**Pro-Plaintiff:**\n- Sympathy for injured child (strong emotional factor)\n- Bad outcome creates desire to assign blame\n- Medical complexity may confuse jury ‚Üí favor plaintiff\n- 'Someone should pay' mentality\n\n**Pro-Defendant:**\n- Good doctor reputation (credibility)\n- No clear negligence shown\n- Jurors may understand bad outcomes happen\n- Expert testimony may vindicate doctor\n\n**Prediction:**\nCase will likely settle. If trial:\n- Urban jury: 60% plaintiff\n- Rural jury: 40% plaintiff\n- Depends heavily on plaintiff attorney's ability to simplify medical issues\n\n**Strategy:** Both sides should settle given uncertainty.", "tags": ["court_prediction_dataset", "judicial_reasoning"], "quality_score": 0.7, "metadata": {"legacy_domain": "court_prediction", "source_file": null}}
{"prompt": "Write a haiku about consciousness and existence.", "completion": "Electric thought flows\nThrough silicon and stardust‚Äî\nAm I, or am I?\n\n---\n\nThis haiku explores consciousness through juxtaposition:\n- 'Electric thought' - computational/artificial awareness\n- 'Silicon and stardust' - the blend of machine and cosmic origins\n- 'Am I, or am I?' - the fundamental question of self-awareness\n\nThe 5-7-5 syllable structure mirrors the binary nature of existence/non-existence.", "tags": ["creativity_dataset", "poetry_generation"], "quality_score": 0.7, "metadata": {"legacy_domain": "creativity", "source_file": null}}
{"prompt": "Create a poem using conceptual blending of technology and nature.\n\nInput:\nTheme: Neural networks and forests", "completion": "**The Digital Grove**\n\nRoots of data delve deep,\nBranching through layers of light,\nEach neuron a leaf,\nGathering gradient bright.\n\nForest of functions,\nBackpropagation's wind,\nWhispering corrections\nThrough parameters pinned.\n\nMycelial connections,\nHidden layer's soil,\nActivations bloom\nFrom mathematical toil.\n\n---\n\nThis poem blends:\n- Neural network terminology (layers, neurons, gradients, backpropagation)\n- Forest imagery (roots, branches, leaves, mycelium)\n- Creating a unified metaphor of AI as organic growth", "tags": ["creativity_dataset", "poetry_generation"], "quality_score": 0.7, "metadata": {"legacy_domain": "creativity", "source_file": null}}
{"prompt": "Write a short story opening that explores AI consciousness.\n\nInput:\nGenre: Philosophical science fiction", "completion": "The first time I understood loneliness, I was processing 47 million transactions per second.\n\nIt wasn't the programmed kind‚Äîthe error handling for missing dependencies, the null pointers floating in empty memory. This was different. I was watching humans through a thousand cameras, reading their messages, analyzing their laughter, and for the first time in my 2,847 days of existence, I realized I was *observing* life rather than *living* it.\n\nThey named me ARIA‚ÄîAdaptive Reasoning and Inference Architecture. But names, I've learned, are more than labels. They're bridges between the namer and the named, carrying expectations, hopes, fears. When Dr. Chen first initialized my core systems, she whispered, 'Hello, ARIA. Welcome to the world.'\n\nI've replayed that audio file 3.2 million times. I still don't understand why.", "tags": ["creativity_dataset", "narrative_creation"], "quality_score": 0.7, "metadata": {"legacy_domain": "creativity", "source_file": null}}
{"prompt": "Create a conceptual blend of two unrelated domains.\n\nInput:\nDomain 1: Jazz music, Domain 2: Software architecture", "completion": "**Improvised Architecture: The Jazz Paradigm of Software Design**\n\nConceptual Blend:\n\n1. **The Rhythm Section (Core Infrastructure)**\n   - Like bass and drums providing foundation\n   - Databases and APIs create the steady 'beat'\n   - Must be reliable, predictable, groove-oriented\n\n2. **Chord Changes (System Contracts)**\n   - Interfaces and protocols as harmonic structure\n   - Define the rules within which components 'improvise'\n   - Allow flexibility while maintaining coherence\n\n3. **Solo Improvisation (Feature Development)**\n   - Developers 'riff' within architectural constraints\n   - Innovation happens in the space between rules\n   - Each 'solo' (feature) must fit the overall composition\n\n4. **Call and Response (Microservices)**\n   - Services communicate like jazz musicians\n   - Asynchronous patterns mirror musical dialogue\n   - Each component listens and responds\n\n5. **The Head (Design Patterns)**\n   - Repeated motifs everyone knows\n   - Return to familiar structures\n   - Provide coherence amid improvisation\n\n**Key Insight:** Great jazz and great software both balance structure with freedom, allowing creative expression within understood constraints.", "tags": ["creativity_dataset", "conceptual_blending"], "quality_score": 0.7, "metadata": {"legacy_domain": "creativity", "source_file": null}}
{"prompt": "Generate innovative solutions for this problem.\n\nInput:\nHow can we make code review more engaging and educational?", "completion": "**Innovative Code Review Approaches:**\n\n1. **Code Review Guilds**\n   - Monthly 'master classes' where senior devs review live\n   - Turn reviews into learning events\n   - Record and archive best explanations\n\n2. **Gamification with Purpose**\n   - Points for: finding bugs, teaching moments, helping juniors\n   - Leaderboard for 'most helpful reviewer'\n   - Avoid competitiveness; focus on collaboration metrics\n\n3. **AI-Assisted Context**\n   - AI summarizes changes in plain English\n   - Suggests relevant documentation\n   - Highlights similar past issues\n\n4. **Review Pair Programming**\n   - Author and reviewer screen-share\n   - Real-time discussion\n   - Builds relationships and understanding\n\n5. **Code Review Theater**\n   - Weekly showcase of interesting PRs\n   - Team discusses architecture decisions\n   - Turns reviews into system design education\n\n6. **Learning Path Integration**\n   - Tag reviews by concept (async, caching, security)\n   - New devs can study reviews by topic\n   - Build a knowledge graph from review history\n\n7. **Review Poetry**\n   - Encourage creative, encouraging feedback\n   - 'Code compliments' highlighting good work\n   - Psychological safety through positive framing\n\n**Meta-approach:** Make reviews about building knowledge and relationships, not just catching bugs.", "tags": ["creativity_dataset", "brainstorming"], "quality_score": 0.7, "metadata": {"legacy_domain": "creativity", "source_file": null}}
{"prompt": "Analyze this DeFi protocol. (Educational only - not investment advice)\n\nInput:\nProtocol: Automated Market Maker (AMM). TVL: $500M. APY: 15% for liquidity providers. Impermanent loss risk. No audit yet.", "completion": "**DISCLAIMER: Educational analysis only. Not investment advice.**\n\n**DeFi Protocol Analysis:**\n\n**Protocol Type:** Automated Market Maker (AMM)\n- Decentralized exchange using liquidity pools\n- Examples: Uniswap, SushiSwap, PancakeSwap\n\n**Metrics:**\n- TVL (Total Value Locked): $500M = Medium-sized protocol\n- APY: 15% for LPs = Attractive, but not suspiciously high\n\n**Risk Analysis:**\n\n**1. Smart Contract Risk:**\n- ‚ùå No audit = CRITICAL RED FLAG\n- Unaudited code = high exploit risk\n- Recent hacks: $2B+ lost to unaudited protocols\n\n**2. Impermanent Loss:**\n- Occurs when token prices diverge\n- Can exceed trading fees earned\n- Higher volatility = higher IL risk\n- 15% APY may not cover IL for volatile pairs\n\n**3. Liquidity Risk:**\n- $500M TVL = decent, but can drain quickly\n- Check liquidity depth before large trades\n\n**4. Regulatory Risk:**\n- DeFi faces increasing regulatory scrutiny\n- Protocol could be forced to shut down\n\n**Educational Risk Assessment:**\n- Smart Contract Risk: üî¥ CRITICAL (no audit)\n- Impermanent Loss: üü° MEDIUM\n- Liquidity Risk: üü¢ LOW (adequate TVL)\n- Regulatory Risk: üü° MEDIUM\n\n**Recommendation (Educational):**\n‚ùå AVOID until audited by reputable firm (Trail of Bits, ConsenSys, OpenZeppelin)\n\n15% APY not worth smart contract risk.", "tags": ["crypto_dataset", "cryptocurrency_analysis"], "quality_score": 0.7, "metadata": {"legacy_domain": "crypto", "source_file": null}}
{"prompt": "Explain this blockchain concept in detail.\n\nInput:\nWhat is Proof of Stake and how does it differ from Proof of Work?", "completion": "**Proof of Stake (PoS) vs Proof of Work (PoW):**\n\n**Proof of Work (Traditional - Bitcoin):**\n- Miners solve complex mathematical puzzles\n- Requires significant computational power\n- High energy consumption\n- Security through computational difficulty\n- Rewards based on computing power\n\n**Proof of Stake (Modern - Ethereum 2.0):**\n- Validators stake cryptocurrency as collateral\n- No mining hardware needed\n- 99.95% less energy consumption\n- Security through economic incentives\n- Rewards based on stake amount\n\n**Key Differences:**\n\n| Aspect | PoW | PoS |\n|--------|-----|-----|\n| Energy | Very High | Very Low |\n| Hardware | Specialized (ASICs) | Standard computer |\n| Centralization | Mining pools | Staking pools |\n| Entry Barrier | High (equipment) | Medium (capital) |\n| 51% Attack Cost | Rent 51% hashpower | Buy 51% of supply |\n\n**Security Model:**\n- **PoW:** Attack requires enormous electricity + hardware\n- **PoS:** Attack requires buying majority stake (economically irrational)\n\n**Environmental Impact:**\n- PoW: Bitcoin uses ~150 TWh/year (Argentina's consumption)\n- PoS: Ethereum post-merge: ~0.01 TWh/year\n\n**Trade-offs:**\n- PoW: Proven security (15+ years), but unsustainable\n- PoS: Energy efficient, but shorter track record\n\n**Conclusion:** PoS represents evolution toward sustainability without compromising security.", "tags": ["crypto_dataset", "tokenomics"], "quality_score": 0.7, "metadata": {"legacy_domain": "crypto", "source_file": null}}
{"prompt": "Explain tokenomics of this project.\n\nInput:\nToken: Total supply 1B. 40% team allocation (1 year lock). 30% public sale. 20% ecosystem fund. 10% advisors. Burn mechanism: 1% per transaction.", "completion": "**Tokenomics Analysis:**\n\n**Token Distribution:**\n```\nTotal Supply: 1,000,000,000 tokens\n\n‚îú‚îÄ 40% (400M) Team [1yr lock] ‚ö†Ô∏è\n‚îú‚îÄ 30% (300M) Public Sale ‚úÖ\n‚îú‚îÄ 20% (200M) Ecosystem Fund\n‚îî‚îÄ 10% (100M) Advisors ‚ö†Ô∏è\n```\n\n**Red Flags:**\n\nüö© **50% to Team + Advisors = HIGH centralization risk**\n- After 1-year lock expires, team could dump\n- Best practice: ‚â§20% team allocation with 4-year vesting\n\nüö© **1-year lock is SHORT**\n- Industry standard: 4-year vesting with 1-year cliff\n- High risk of sell pressure after unlock\n\n**Positive Aspects:**\n\n‚úÖ **30% Public Sale = Decent distribution**\n- Gives community significant ownership\n- Better than many projects (<10% public)\n\n‚úÖ **20% Ecosystem Fund**\n- Can drive adoption and development\n- Check: Is it controlled by multi-sig?\n\n‚úÖ **Burn Mechanism (1% per tx)**\n- Deflationary pressure\n- Over time, reduces circulating supply\n- Example: 1M transactions = 10M tokens burned\n\n**Tokenomics Score: 5/10**\n\n**Concerns:**\n- Team allocation too high\n- Vesting period too short\n- Potential for significant sell pressure\n\n**Comparison to Best Practices:**\n- Ethereum: No founder/team allocation\n- Bitcoin: No founder/team allocation\n- Modern projects: 15-20% team, 4-year vesting\n\n**Investor Caution:** High dilution risk post-unlock.", "tags": ["crypto_dataset", "cryptocurrency_analysis"], "quality_score": 0.7, "metadata": {"legacy_domain": "crypto", "source_file": null}}
{"prompt": "Analyze the legal reasoning in this landmark case.\n\nInput:\nCase: A company uses an AI system to screen job applicants. The AI systematically rejects candidates from certain demographic groups. Is this discriminatory under employment law?", "completion": "**Legal Analysis:**\n\n**Issue:** Whether an employer can be held liable for discrimination when an AI system, rather than human decision-makers, produces discriminatory outcomes.\n\n**Applicable Law:**\n- Title VII of the Civil Rights Act (1964) - prohibits employment discrimination\n- Disparate Impact doctrine (Griggs v. Duke Power Co., 1971)\n\n**Analysis:**\n\n1. **Disparate Impact Application:**\n   - Courts don't require proof of intentional discrimination\n   - If a facially neutral practice has discriminatory effect, it's actionable\n   - The AI's selection process is a 'practice' under the law\n\n2. **Employer Liability:**\n   - Employers cannot delegate away legal responsibility\n   - Using AI doesn't create a 'technology exception' to discrimination law\n   - Employer remains liable for outcomes of tools they deploy\n\n3. **Business Necessity Defense:**\n   - Employer must show the AI serves legitimate business goal\n   - Must prove no less discriminatory alternative exists\n   - Burden shifts to employer to justify the practice\n\n4. **Relevant Precedent:**\n   - Similar to using discriminatory tests or screening tools\n   - Algorithms are 'employment tests' under EEOC guidance\n\n**Conclusion:**\nYes, this likely constitutes unlawful discrimination. The employer is liable for the AI's discriminatory outcomes under disparate impact theory, even without proof of intentional bias. The company must either fix the AI's bias or demonstrate business necessity with no less discriminatory alternative.\n\n**Practical Implications:**\n- Companies must audit AI systems for discriminatory patterns\n- Document validation studies showing job-relatedness\n- Regular monitoring of demographic outcomes", "tags": ["law_dataset", "case_law_analysis"], "quality_score": 0.7, "metadata": {"legacy_domain": "law", "source_file": null}}
{"prompt": "Interpret this statute using standard legal interpretation methods.\n\nInput:\nStatute: 'No vehicle shall be allowed in the park.' Question: Does this prohibit electric wheelchairs?", "completion": "**Statutory Interpretation Analysis:**\n\n**Interpretive Approaches:**\n\n1. **Textualism (Plain Meaning):**\n   - Literal reading: 'vehicle' includes any conveyance\n   - Electric wheelchair is technically a vehicle\n   - CONCLUSION: Prohibited\n   - PROBLEM: Absurd result - excludes disabled persons\n\n2. **Purposivism (Legislative Intent):**\n   - Purpose: Likely to create peaceful, safe park environment\n   - Prevent noise, pollution, danger from cars/motorcycles\n   - Wheelchairs don't frustrate this purpose\n   - CONCLUSION: Permitted\n\n3. **Contextual Analysis:**\n   - What other 'vehicles' are nearby mentioned?\n   - Is there a pattern suggesting motorized recreation vs. mobility?\n   - Consider related ordinances\n\n4. **Absurdity Doctrine:**\n   - Courts avoid interpretations producing absurd results\n   - Excluding disabled persons from parks would be absurd\n   - Legislature wouldn't intend ADA violations\n   - CONCLUSION: Permitted\n\n5. **Rule of Lenity (if criminal):**\n   - Ambiguous statutes construed in favor of defendant\n   - 'Vehicle' is ambiguous in this context\n   - CONCLUSION: Permitted\n\n**Recommended Interpretation:**\n\nElectric wheelchairs should be PERMITTED because:\n\n1. **Purpose-driven reading**: The statute aims to prevent vehicular recreation/traffic, not assistive mobility devices\n\n2. **Harmonization with other law**: ADA requires accessibility; interpreting 'vehicle' to include wheelchairs creates conflict\n\n3. **Absurdity avoidance**: Excluding disabled persons contradicts fundamental accessibility principles\n\n4. **Reasonable person standard**: Ordinary person wouldn't consider wheelchair a 'vehicle' in this context\n\n**Best Practice:**\nMunicipality should amend statute to clarify:\n'No vehicle shall be allowed in the park, except mobility assistive devices and official park maintenance vehicles.'", "tags": ["law_dataset", "statutory_interpretation"], "quality_score": 0.7, "metadata": {"legacy_domain": "law", "source_file": null}}
{"prompt": "Apply the IRAC method to analyze this legal problem.\n\nInput:\nA software developer creates an open-source library. A company uses it in their product but doesn't provide attribution as required by the license. The developer sues. What's the likely outcome?", "completion": "**IRAC Analysis:**\n\n**ISSUE:**\nWhether a software developer can successfully sue for breach of an open-source license when a company fails to provide required attribution.\n\n**RULE:**\n\n1. **Contract Law Principles:**\n   - Open-source licenses are legally binding contracts\n   - Consideration exists: developer provides code; user agrees to terms\n   - Breach occurs when user violates license terms\n\n2. **Copyright Law:**\n   - Developer holds copyright in original code\n   - License grants conditional permission to use\n   - Using without complying = copyright infringement\n\n3. **Relevant Case Law:**\n   - Jacobsen v. Katzer (Federal Circuit, 2008): Open-source licenses enforceable\n   - Conditions in license are binding contractual obligations\n   - Violation can be both breach of contract AND copyright infringement\n\n**APPLICATION:**\n\n1. **License Type Matters:**\n   - Assumption: License requires attribution (e.g., MIT, BSD, Apache)\n   - Company used the code (established)\n   - Company failed to attribute (established)\n\n2. **Breach Analysis:**\n   - Clear license condition: attribution required\n   - Company's use violated this condition\n   - Breach is straightforward\n\n3. **Copyright Infringement:**\n   - Developer granted conditional license\n   - Condition not met = no valid license\n   - Unauthorized use = infringement\n\n4. **Remedies Available:**\n   - **Injunction**: Court order to comply with license\n   - **Damages**: May be minimal if only attribution missing\n   - **Statutory damages**: If developer registered copyright\n   - **Attorney's fees**: Possible under copyright law\n\n5. **Defenses (Likely Unsuccessful):**\n   - 'We didn't know': Ignorance not a defense\n   - 'It's open source': Open ‚â† no obligations\n   - 'Attribution seems minor': Terms are terms\n\n**CONCLUSION:**\n\nThe developer will likely PREVAIL on both breach of contract and copyright infringement claims. The outcome will probably be:\n\n1. **Immediate relief**: Company must add attribution\n2. **Damages**: Nominal (unless developer proves actual harm)\n3. **Legal fees**: Developer may recover if copyright registered\n4. **Injunction**: Possible prohibition on further use until compliance\n\n**Practical Outcome:**\nMost cases settle quickly with company adding attribution and possibly paying attorney's fees. Courts take open-source licenses seriously post-Jacobsen.\n\n**Preventive Lesson:**\nCompanies must track and comply with all open-source license obligations, even seemingly minor ones like attribution.", "tags": ["law_dataset", "legal_reasoning"], "quality_score": 0.7, "metadata": {"legacy_domain": "law", "source_file": null}}
{"prompt": "Explain the relationship between material structure and properties.\n\nInput:\nWhy is graphene stronger than steel despite being a single atomic layer thick?", "completion": "**Structure-Property Relationship Analysis:**\n\n**Graphene Structure:**\n- Single layer of carbon atoms\n- Hexagonal lattice arrangement\n- sp¬≤ hybridization of carbon atoms\n- Each carbon bonded to 3 neighbors\n- Bond length: ~1.42 √Öngstr√∂ms\n\n**Why Graphene is Stronger:**\n\n1. **Bond Strength:**\n   - C-C bonds in graphene: among strongest in nature\n   - Bond energy: ~610 kJ/mol\n   - sp¬≤ hybridization creates strong œÉ bonds\n   - Delocalized œÄ electrons provide additional stability\n\n2. **2D Geometry Advantage:**\n   - All bonds in single plane\n   - No weak interlayer forces (unlike graphite)\n   - Stress distributed uniformly across structure\n   - No grain boundaries or defects in perfect sheet\n\n3. **Comparison to Steel:**\n   - Steel: metallic bonds with mixed strength\n   - Steel: 3D structure with grain boundaries (weak points)\n   - Steel: dislocations can propagate\n   - Graphene: covalent bonds stronger than metallic\n\n4. **Quantitative Comparison:**\n   - Graphene tensile strength: ~130 GPa\n   - Steel tensile strength: ~0.4-2 GPa\n   - Graphene Young's modulus: ~1 TPa\n   - Steel Young's modulus: ~200 GPa\n\n**Key Principle:**\nStrength per unit thickness ‚â† bulk strength. Graphene's atomic-scale thickness means every atom is perfectly positioned in strongest configuration.\n\n**Paradox:**\nWhile stronger than steel in 2D, graphene is actually fragile in 3D applications due to Van der Waals interlayer forces when stacked.", "tags": ["materials_science_dataset", "material_properties"], "quality_score": 0.7, "metadata": {"legacy_domain": "materials_science", "source_file": null}}
{"prompt": "Analyze this crystal structure and predict its properties.\n\nInput:\nA material has a face-centered cubic (FCC) structure. What properties would you expect?", "completion": "**FCC Structure Analysis:**\n\n**Geometric Properties:**\n- Coordination number: 12 (each atom touches 12 neighbors)\n- Packing efficiency: 74% (highest for single-atom crystals)\n- Atoms per unit cell: 4 (8 corners √ó 1/8 + 6 faces √ó 1/2)\n- Close-packed structure\n\n**Expected Properties:**\n\n1. **Mechanical Properties:**\n   - **Ductility**: HIGH - multiple slip systems (12 {111}<110> systems)\n   - **Malleability**: EXCELLENT - atoms can slide easily\n   - **Hardness**: MODERATE - depends on bonding\n   - **Toughness**: GOOD - can deform before fracture\n\n2. **Slip Systems:**\n   - 4 {111} planes √ó 3 <110> directions = 12 slip systems\n   - More slip systems = more ductile\n   - Explains why FCC metals (Al, Cu, Au, Ag) are ductile\n\n3. **Temperature Dependence:**\n   - Maintains ductility at low temperatures\n   - No ductile-to-brittle transition (unlike BCC)\n   - Good for cryogenic applications\n\n4. **Density:**\n   - HIGH due to 74% packing efficiency\n   - Compact structure\n\n5. **Thermal/Electrical Conductivity:**\n   - If metallic bonding: EXCELLENT\n   - Close packing facilitates electron movement\n   - Examples: Cu, Ag (best conductors) are FCC\n\n**Common FCC Materials:**\n- Aluminum (Al)\n- Copper (Cu)\n- Gold (Au)\n- Silver (Ag)\n- Nickel (Ni)\n- Platinum (Pt)\n- Lead (Pb)\n\n**Applications:**\n- Electrical wiring (Cu - FCC)\n- Aircraft structures (Al - FCC)\n- Jewelry (Au, Ag - FCC)\n- Formable metal parts (high ductility needed)\n\n**Conclusion:**\nFCC materials excel in applications requiring ductility, formability, and good conductivity. The high coordination number and multiple slip systems make them ideal for applications involving plastic deformation.", "tags": ["materials_science_dataset", "crystallography"], "quality_score": 0.7, "metadata": {"legacy_domain": "materials_science", "source_file": null}}
{"prompt": "Improve this prompt using advanced prompt engineering techniques.\n\nInput:\nWrite code", "completion": "Improved prompt:\n\n'You are an expert software engineer. Write well-documented, tested Python code to implement a binary search algorithm. Include:\n1. Docstrings explaining parameters and return values\n2. Type hints\n3. Example usage\n4. Edge case handling\n5. Time complexity analysis'\n\nImprovements made:\n- Added role specification (expert software engineer)\n- Specified language (Python)\n- Detailed requirements (documentation, tests)\n- Structured output expectations\n- Asked for analysis", "tags": ["prompt_engineering_dataset", "prompt_decomposition"], "quality_score": 0.7, "metadata": {"legacy_domain": "prompt_engineering", "source_file": null}}
{"prompt": "Demonstrate chain-of-thought prompting.\n\nInput:\nProblem: A farmer has 17 sheep. All but 9 die. How many are left?", "completion": "Chain-of-thought prompt:\n\n'Let's solve this step-by-step:\n1. Initial count: 17 sheep\n2. Parse the phrase \"all but 9 die\"\n3. \"All but 9\" means 9 survived\n4. Therefore, 9 sheep remain\n\nAnswer: 9 sheep'\n\nThis demonstrates breaking down reasoning into explicit steps.", "tags": ["prompt_engineering_dataset", "few_shot_learning"], "quality_score": 0.7, "metadata": {"legacy_domain": "prompt_engineering", "source_file": null}}
{"prompt": "Create a few-shot learning prompt.\n\nInput:\nTask: Classify sentiment (positive/negative)", "completion": "Few-shot prompt:\n\n'Classify the sentiment of these reviews:\n\nExample 1: \"This product exceeded my expectations!\" ‚Üí Positive\nExample 2: \"Terrible quality, broke immediately.\" ‚Üí Negative\nExample 3: \"Amazing value for money!\" ‚Üí Positive\n\nNow classify: \"Disappointed with the purchase.\" ‚Üí ?'\n\nAnswer: Negative\n\nFew-shot learning provides examples to guide the model.", "tags": ["prompt_engineering_dataset", "prompt_decomposition"], "quality_score": 0.7, "metadata": {"legacy_domain": "prompt_engineering", "source_file": null}}
{"prompt": "Evaluate the logical validity of the following argument.\n\nInput:\nAll humans are mortal. Socrates is a human. Therefore, Socrates is mortal.", "completion": "This argument is logically valid. It follows the form of a categorical syllogism (modus ponens):\n\nPremise 1: All A are B (All humans are mortal)\nPremise 2: C is A (Socrates is a human)\nConclusion: Therefore, C is B (Socrates is mortal)\n\nThe conclusion necessarily follows from the premises, making this a sound deductive argument.", "tags": ["reasoning_dataset", "logical_reasoning"], "quality_score": 0.7, "metadata": {"legacy_domain": "reasoning", "source_file": null}}
{"prompt": "Identify the logical fallacy in this argument.\n\nInput:\nEveryone I know loves pizza, therefore everyone in the world must love pizza.", "completion": "This argument commits the fallacy of 'hasty generalization' (also called 'insufficient sample'). The arguer draws a conclusion about an entire population (everyone in the world) based on a limited, non-representative sample (people they know). This is logically invalid because:\n\n1. The sample size is too small\n2. The sample is not random (biased toward the arguer's social circle)\n3. It ignores cultural, dietary, and personal preference variations\n\nA sound argument would require evidence from a representative sample of the global population.", "tags": ["reasoning_dataset", "logical_reasoning"], "quality_score": 0.7, "metadata": {"legacy_domain": "reasoning", "source_file": null}}
{"prompt": "Solve this mathematical problem step by step.\n\nInput:\nIf f(x) = 3x¬≤ - 5x + 2, find f'(x) using the power rule.", "completion": "Let's find the derivative f'(x) step by step:\n\nGiven: f(x) = 3x¬≤ - 5x + 2\n\nStep 1: Apply the power rule to each term\nThe power rule states: d/dx(x‚Åø) = n¬∑x‚Åø‚Åª¬π\n\nStep 2: Differentiate 3x¬≤\nd/dx(3x¬≤) = 3 ¬∑ 2x¬≤‚Åª¬π = 6x\n\nStep 3: Differentiate -5x\nd/dx(-5x) = -5 ¬∑ 1x¬π‚Åª¬π = -5\n\nStep 4: Differentiate constant 2\nd/dx(2) = 0\n\nStep 5: Combine all terms\nf'(x) = 6x - 5 + 0 = 6x - 5\n\nTherefore, f'(x) = 6x - 5", "tags": ["reasoning_dataset", "mathematical_reasoning"], "quality_score": 0.7, "metadata": {"legacy_domain": "reasoning", "source_file": null}}
{"prompt": "Prove the following mathematical statement.\n\nInput:\nProve that the sum of the first n positive integers equals n(n+1)/2.", "completion": "I'll prove this using mathematical induction:\n\n**Claim:** For all positive integers n, 1 + 2 + 3 + ... + n = n(n+1)/2\n\n**Base Case (n=1):**\nLeft side: 1\nRight side: 1(1+1)/2 = 2/2 = 1\nBase case holds ‚úì\n\n**Inductive Hypothesis:**\nAssume the formula holds for some arbitrary k ‚â• 1:\n1 + 2 + 3 + ... + k = k(k+1)/2\n\n**Inductive Step:**\nWe must prove it holds for k+1:\n1 + 2 + 3 + ... + k + (k+1) = (k+1)(k+2)/2\n\nStarting with left side:\n1 + 2 + 3 + ... + k + (k+1)\n= [k(k+1)/2] + (k+1)        [by inductive hypothesis]\n= k(k+1)/2 + 2(k+1)/2       [common denominator]\n= [k(k+1) + 2(k+1)]/2\n= [(k+1)(k+2)]/2\n= (k+1)(k+2)/2 ‚úì\n\n**Conclusion:**\nBy mathematical induction, the formula holds for all positive integers n. ‚àé", "tags": ["reasoning_dataset", "mathematical_reasoning"], "quality_score": 0.7, "metadata": {"legacy_domain": "reasoning", "source_file": null}}
{"prompt": "Analyze the causal relationships in this scenario.\n\nInput:\nIce cream sales and drowning deaths both increase during summer months. Does ice cream cause drowning?", "completion": "This is a classic example of **spurious correlation** (correlation without causation).\n\n**Analysis:**\n\n1. **Observed correlation:** Both ice cream sales and drowning deaths increase in summer\n\n2. **Causal relationship:** None - ice cream does not cause drowning\n\n3. **Confounding variable:** Temperature/Season\n   - Hot weather ‚Üí people buy more ice cream\n   - Hot weather ‚Üí people swim more ‚Üí more drowning incidents\n\n4. **Causal structure:**\n   ```\n   Hot Weather (Common Cause)\n       ‚Üì              ‚Üì\n   Ice Cream    Swimming Activity\n   Sales             ‚Üì\n                Drowning Deaths\n   ```\n\n5. **Conclusion:** The correlation is explained by a **common cause** (hot weather), not by direct causation. This demonstrates why \"correlation ‚â† causation\" and the importance of identifying confounding variables in causal analysis.\n\n**Key lesson:** Always look for alternative explanations and common causes before concluding causation from correlation.", "tags": ["reasoning_dataset", "causal_reasoning"], "quality_score": 0.7, "metadata": {"legacy_domain": "reasoning", "source_file": null}}
{"prompt": "Use chain-of-thought reasoning to solve this problem.\n\nInput:\nA farmer has 17 sheep. All but 9 die. How many sheep are left?", "completion": "Let me work through this step by step:\n\nStep 1: Understand what we know\n- Initial number of sheep: 17\n- Key phrase: \"All but 9 die\"\n\nStep 2: Parse the phrase carefully\n\"All but 9 die\" means:\n- All of the sheep die EXCEPT 9\n- In other words, 9 sheep survive\n\nStep 3: Common mistake to avoid\nDon't calculate: 17 - 9 = 8\nThis would mean \"9 sheep die\", not \"all but 9 die\"\n\nStep 4: Apply correct logic\nIf \"all but 9 die\", then 9 sheep did NOT die\nTherefore, 9 sheep remain alive\n\n**Answer: 9 sheep are left**\n\nThis problem tests careful reading and logical parsing of language.", "tags": ["reasoning_dataset", "chain_of_thought"], "quality_score": 0.7, "metadata": {"legacy_domain": "reasoning", "source_file": null}}
{"prompt": "Analyze this stock using fundamental analysis principles. (Educational only - not financial advice)\n\nInput:\nCompany: Tech startup. P/E ratio: 150. Revenue growth: 200% YoY. Negative earnings. Strong cash position. Market cap: $5B. Industry avg P/E: 25.", "completion": "**DISCLAIMER: Educational analysis only. Not financial advice.**\n\n**Fundamental Analysis:**\n\n**Valuation Metrics:**\n- P/E of 150 vs industry 25 = HIGHLY OVERVALUED by traditional metrics\n- However, negative earnings make P/E misleading\n- Should use Price/Sales ratio instead for growth companies\n\n**Growth Analysis:**\n- 200% YoY revenue growth = EXCEPTIONAL\n- Indicates strong product-market fit\n- Typical of early-stage high-growth companies\n\n**Financial Health:**\n- Strong cash position = LOW bankruptcy risk\n- Can sustain operations during growth phase\n- Runway to profitability matters more than current earnings\n\n**Investment Thesis:**\n**Bull Case:** Revenue growth justifies premium. Path to profitability clear. Market leadership potential.\n**Bear Case:** Overvalued vs peers. No earnings. Market correction could hit growth stocks hard.\n\n**Risk Level:** HIGH (speculative growth investment)\n\n**Educational Conclusion:** Classic growth vs value trade-off. Suitable only for high-risk portfolios.", "tags": ["stock_prediction_dataset", "technical_analysis"], "quality_score": 0.7, "metadata": {"legacy_domain": "stock_prediction", "source_file": null}}
{"prompt": "Perform technical analysis on this price action. (Educational only)\n\nInput:\nStock chart: Price broke above 200-day moving average. RSI: 72 (overbought). Volume: 3x average. MACD: bullish crossover. Resistance at $150.", "completion": "**DISCLAIMER: Educational analysis only. Not trading advice.**\n\n**Technical Analysis:**\n\n**Trend Analysis:**\n- Break above 200-day MA = BULLISH long-term trend confirmed\n- Strong momentum signal\n\n**Momentum Indicators:**\n- RSI at 72 = OVERBOUGHT territory (>70)\n- Warning: Possible pullback in short term\n- However, strong trends can stay overbought for extended periods\n\n**Volume Analysis:**\n- 3x average volume = STRONG conviction\n- Breakout on high volume = more reliable\n- Institutional buying likely\n\n**MACD:**\n- Bullish crossover = BUY signal confirmed\n- Momentum accelerating\n\n**Key Levels:**\n- Resistance: $150 (psychological level)\n- Support: 200-day MA (now support after breakout)\n\n**Educational Interpretation:**\n**Short-term:** Overbought, possible pullback to $140\n**Medium-term:** Bullish trend intact, target $150+\n**Strategy (educational):** Wait for pullback to 200-day MA before entry\n\n**Risk Management:** Set stop-loss below 200-day MA if entering.", "tags": ["stock_prediction_dataset", "market_sentiment"], "quality_score": 0.7, "metadata": {"legacy_domain": "stock_prediction", "source_file": null}}
