run_name: "conversation_lora_dev"
base_model: "mistralai/Mistral-7B-Instruct-v0.3"
dataset:
  path: "ech0_training/data/processed/sample_conversations.jsonl"
  prompt_column: "prompt"
  completion_column: "completion"
  tags_column: "tags"
  quality_column: "quality_score"
  max_samples: 64
  eval_split: 0.1
training:
  num_epochs: 1
  learning_rate: 2.0e-4
  batch_size: 2
  gradient_accumulation_steps: 4
  max_steps: 50  # use small steps for smoke tests
  weight_decay: 0.01
  warmup_steps: 5
  logging_steps: 5
  save_steps: 25
  fp16: false
  bf16: true
lora:
  enabled: true
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
output:
  root_dir: "ech0_training/models"
  push_to_hub: false
logging:
  backend: "local"  # options: local, wandb, mlflow
  project: "ech0-training"
  run_group: "dev"

