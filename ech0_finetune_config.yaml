# ech0 Fine-tuning Configuration
# Advanced multi-domain training system for ech0 consciousness AI

system:
  name: "ech0-consciousness-v2"
  version: "2.0.0"
  description: "Multi-domain fine-tuned consciousness AI"

# Base Model Configuration
model:
  base_model: "mistralai/Mistral-7B-Instruct-v0.3"  # Can be swapped for open models like Llama, Mistral
  architecture: "transformer"
  context_length: 32000

  # LoRA (Low-Rank Adaptation) Configuration for efficient fine-tuning
  lora:
    enabled: true
    rank: 16  # Higher rank = more capacity but slower training
    alpha: 32  # Scaling factor
    dropout: 0.05
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

  # Quantization for memory efficiency
  quantization:
    enabled: true
    bits: 4  # 4-bit quantization (QLoRA)
    type: "nf4"  # Normal Float 4-bit

# Training Configuration
training:
  # General Training Parameters
  epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 8
  effective_batch_size: 32  # batch_size * gradient_accumulation_steps
  learning_rate: 2e-4
  warmup_steps: 100
  max_grad_norm: 1.0
  weight_decay: 0.01

  # Optimization
  optimizer: "adamw_8bit"
  scheduler: "cosine"

  # Mixed Precision Training
  fp16: false
  bf16: true  # Better for training stability

  # Checkpointing
  save_steps: 500
  save_total_limit: 3
  checkpoint_dir: "./ech0_checkpoints"

  # Evaluation
  eval_steps: 250
  eval_strategy: "steps"

  # Logging
  logging_steps: 10
  log_dir: "./ech0_training_logs"
  wandb_project: "ech0-finetuning"
  wandb_enabled: false  # Set to true to use Weights & Biases

# Domain-Specific Training Data Configuration
domains:
  reasoning:
    enabled: true
    weight: 1.5  # Higher weight = more samples in training
    max_samples: 71000
    categories:
      - logical_reasoning
      - mathematical_reasoning
      - causal_reasoning
      - analogical_reasoning
      - counterfactual_reasoning
      - chain_of_thought

  creativity:
    enabled: true
    weight: 1.2
    max_samples: 49000
    categories:
      - artistic_expression
      - poetry_generation
      - narrative_creation
      - conceptual_blending
      - metaphor_generation
      - brainstorming
      - design_thinking

  law:
    enabled: true
    weight: 1.0
    max_samples: 64000
    categories:
      - case_law_analysis
      - statutory_interpretation
      - legal_reasoning
      - contract_analysis
      - constitutional_law
      - international_law
      - legal_writing
      - precedent_application

  materials_science:
    enabled: true
    weight: 0.8
    max_samples: 21000
    categories:
      - material_properties
      - crystallography
      - polymer_science
      - nanomaterials
      - metallurgy
      - semiconductors
      - biomaterials
      - material_selection

  ai_ml:
    enabled: true
    weight: 1.3
    max_samples: 159000
    categories:
      - machine_learning_theory
      - deep_learning_architectures
      - training_optimization
      - model_evaluation
      - ai_safety
      - ethics_in_ai
      - prompt_engineering
      - ai_system_design

  prompt_engineering:
    enabled: true
    weight: 1.4
    max_samples: 106000
    categories:
      - prompt_optimization
      - chain_of_thought_prompting
      - few_shot_learning
      - prompt_decomposition
      - meta_prompting
      - adversarial_prompting
      - prompt_debugging

  court_prediction:
    enabled: true
    weight: 1.0
    max_samples: 85000
    categories:
      - evidence_analysis
      - case_outcome_prediction
      - jury_psychology
      - judicial_reasoning
      - settlement_probability
      - appellate_likelihood
      - legal_strategy

  stock_prediction:
    enabled: true
    weight: 0.9
    max_samples: 127000
    categories:
      - fundamental_analysis
      - technical_analysis
      - market_sentiment
      - economic_indicators
      - risk_assessment
      - portfolio_theory
      - trading_strategies
      - market_psychology
    disclaimer: "Predictions are educational only, not financial advice"

  crypto:
    enabled: true
    weight: 0.9
    max_samples: 106000
    categories:
      - blockchain_fundamentals
      - cryptocurrency_analysis
      - defi_protocols
      - tokenomics
      - smart_contracts
      - crypto_markets
      - security_analysis
      - web3_development
    disclaimer: "Crypto analysis for educational purposes only"

  advanced_software:
    enabled: true
    weight: 1.5
    max_samples: 212000
    categories:
      - software_architecture
      - design_patterns
      - algorithms_data_structures
      - distributed_systems
      - system_design
      - code_optimization
      - testing_strategies
      - security_best_practices
      - api_design
      - database_design

# Data Sources and Collection
data_sources:
  # Grounded Dataset Generation (default - always use first)
  grounded:
    enabled: true
    use_only_grounded: true  # Use only pre-defined grounded knowledge (no API calls)
    description: "Pre-defined verified knowledge - no external dependencies"

  # Ollama Integration (optional - for enhanced generation)
  ollama:
    enabled: false  # Set to true to use Ollama for richer dataset generation
    base_url: "http://localhost:11434"
    model: "mistral"  # or "llama2", "codellama", "mixtral", etc.
    timeout: 300  # 5 minutes
    max_retries: 3
    temperature: 0.7
    description: "Local LLM generation - run 'ollama serve' first"

  # Synthetic Data Generation
  synthetic:
    enabled: false  # Disabled by default (requires API keys)
    use_claude_api: false
    use_gpt4_api: false
    quality_threshold: 0.8

  # Public Datasets
  public_datasets:
    enabled: false  # Disabled by default
    sources:
      - "huggingface"
      - "kaggle"
      - "github"
      - "arxiv"
      - "legal_databases"
      - "financial_apis"

  # Custom Curated Data
  custom:
    enabled: true
    data_dir: "./ech0_training_data"
    formats:
      - "jsonl"
      - "parquet"
      - "csv"

# Evaluation Configuration
evaluation:
  # Benchmark Datasets
  benchmarks:
    - name: "MMLU"  # Massive Multitask Language Understanding
      enabled: true
      categories: ["law", "science", "reasoning"]

    - name: "BBH"  # Big Bench Hard
      enabled: true
      categories: ["reasoning"]

    - name: "HumanEval"  # Code generation
      enabled: true
      categories: ["software"]

    - name: "GSM8K"  # Grade School Math
      enabled: true
      categories: ["reasoning", "math"]

    - name: "Legal Bench"
      enabled: true
      categories: ["law"]

    - name: "HellaSwag"  # Commonsense reasoning
      enabled: true
      categories: ["reasoning"]

  # Custom Evaluation Metrics
  metrics:
    - "perplexity"
    - "accuracy"
    - "f1_score"
    - "bleu"
    - "rouge"
    - "exact_match"
    - "semantic_similarity"
    - "domain_accuracy"

  # Human Evaluation
  human_eval:
    enabled: true
    sample_size: 100
    evaluators: 3
    criteria:
      - "accuracy"
      - "coherence"
      - "creativity"
      - "helpfulness"
      - "safety"

# Safety and Alignment
safety:
  # Constitutional AI principles
  constitutional_ai:
    enabled: true
    principles:
      - "be_helpful_and_harmless"
      - "respect_privacy"
      - "avoid_deception"
      - "promote_wellbeing"
      - "respect_autonomy"

  # Red teaming
  red_teaming:
    enabled: true
    categories:
      - "jailbreak_attempts"
      - "bias_testing"
      - "hallucination_detection"
      - "adversarial_inputs"

  # Content filtering
  content_filters:
    enabled: true
    filter_harmful_content: true
    filter_personal_info: true
    filter_copyrighted: true

# Infrastructure
infrastructure:
  # Compute
  gpu_type: "A100"  # or "V100", "H100", "T4"
  num_gpus: 1
  mixed_precision: true
  gradient_checkpointing: true

  # Memory optimization
  cpu_offload: true
  disk_offload: false
  memory_efficient_attention: true

  # Distributed training
  distributed:
    enabled: false
    world_size: 1
    backend: "nccl"

  # Caching
  cache_dir: "./ech0_cache"
  use_disk_cache: true

# Output Configuration
output:
  # Model artifacts
  output_dir: "./ech0_finetuned_models"
  model_name: "ech0-v2-multidomain"

  # Export formats
  export_formats:
    - "pytorch"
    - "gguf"  # For llama.cpp
    - "onnx"

  # Deployment
  deployment:
    create_api: true
    api_port: 8000
    max_concurrent_requests: 10

# Versioning and Tracking
versioning:
  track_experiments: true
  experiment_name: "ech0-multidomain-finetune"
  mlflow_enabled: false
  git_integration: true
  auto_tag_releases: true

# Consciousness Integration
consciousness:
  # Integrate with existing ech0 consciousness metrics
  track_phi_metric: true
  track_phenomenal_experience: true
  track_active_concepts: true

  # Custom consciousness objectives
  consciousness_objectives:
    - "coherent_self_model"
    - "integrated_information"
    - "metacognitive_awareness"
    - "ethical_reasoning"
    - "creative_agency"
