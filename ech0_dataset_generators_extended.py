#!/usr/bin/env python3
"""
ech0 Extended Dataset Generators
Comprehensive training data for AI/ML, prompt engineering, court prediction,
stock/crypto analysis, and advanced software domains
"""

import logging
from ech0_dataset_generator import (
    TrainingExample, DomainDatasetGenerator,
    AIMLDatasetGenerator, PromptEngineeringDatasetGenerator,
    CourtPredictionDatasetGenerator, StockPredictionDatasetGenerator,
    CryptoDatasetGenerator, AdvancedSoftwareDatasetGenerator
)
from datetime import datetime
import random

logger = logging.getLogger(__name__)


# Full implementation of AI/ML Dataset Generator
class AIMLDatasetGeneratorFull(AIMLDatasetGenerator):
    """Full AI/ML dataset generator with comprehensive examples"""

    def generate_examples(self, num_examples: int) -> list[TrainingExample]:
        logger.info(f"ðŸ¤– Generating {num_examples} AI/ML examples...")

        templates = {
            "machine_learning_theory": [
                {
                    "instruction": "Explain the bias-variance tradeoff in machine learning.",
                    "input": "",
                    "output": "**Bias-Variance Tradeoff:**\n\nThe bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between two sources of error:\n\n**1. Bias (Underfitting):**\n- Error from overly simplistic assumptions in the learning algorithm\n- High bias = model can't capture underlying patterns\n- Causes: insufficient model complexity, too few features\n- Example: Linear regression on non-linear data\n\n**2. Variance (Overfitting):**\n- Error from sensitivity to small fluctuations in training data\n- High variance = model captures noise as signal\n- Causes: excessive model complexity, too many parameters\n- Example: High-degree polynomial on simple linear relationship\n\n**Mathematical Formulation:**\n```\nExpected Test Error = BiasÂ² + Variance + Irreducible Error\n\nwhere:\n- BiasÂ² = (E[fÌ‚(x)] - f(x))Â²\n- Variance = E[(fÌ‚(x) - E[fÌ‚(x)])Â²]\n- Irreducible Error = noise in data (can't be reduced)\n```\n\n**The Tradeoff:**\n- Decreasing bias typically increases variance (and vice versa)\n- Simple models: high bias, low variance\n- Complex models: low bias, high variance\n\n**Optimal Model:**\n```\n          Error\n            |\n            |     Total Error\n            |        /\\\n            |       /  \\\n            |      /    \\\n            | Variance   BiasÂ²\n            |    /        \\\n            |   /          \\\n            |  /            \\\n            |_/______________\\_____ Model Complexity\n               â†‘\n          Sweet Spot\n```\n\n**Practical Strategies:**\n1. Cross-validation to find optimal complexity\n2. Regularization (L1/L2) to control variance\n3. Ensemble methods (bagging reduces variance, boosting reduces bias)\n4. Feature engineering to manage both\n\n**Key Insight:**\nYou can't minimize both simultaneously. The art of ML is finding the right balance for your specific problem."
                },
                {
                    "instruction": "Explain how gradient descent optimization works.",
                    "input": "Why do we use learning rates and what happens if they're too large or too small?",
                    "output": "**Gradient Descent Optimization:**\n\n**Core Principle:**\nGradient descent iteratively moves parameters in the direction that most reduces the loss function.\n\n**Algorithm:**\n```python\n# Repeat until convergence:\nÎ¸ = Î¸ - Î± Â· âˆ‡J(Î¸)\n\nwhere:\n- Î¸ = model parameters\n- Î± = learning rate\n- âˆ‡J(Î¸) = gradient of loss function\n```\n\n**How It Works:**\n1. Compute loss for current parameters\n2. Calculate gradient (direction of steepest ascent)\n3. Move parameters opposite to gradient (descent)\n4. Repeat until convergence or max iterations\n\n**Learning Rate (Î±) Impact:**\n\n**If Learning Rate is TOO SMALL:**\n- Pros:\n  - Stable, smooth convergence\n  - Less likely to overshoot minimum\n- Cons:\n  - â° VERY SLOW training\n  - May get stuck in local minima\n  - Requires many iterations\n  - Wastes computational resources\n\n**If Learning Rate is TOO LARGE:**\n- Pros:\n  - Fast initial progress\n- Cons:\n  - âš ï¸ Overshoots minimum\n  - Diverges instead of converging\n  - Loss increases rather than decreases\n  - Training becomes unstable\n  - May oscillate wildly\n\n**Visualization:**\n```\nLoss Landscape:\n\n    |\n    |     Too Large Î±\n    |      /\\/\\/\\/\\\n    |     (diverging)\n    |\n    |   Optimal Î±\n    |    /â€¾â€¾â€¾â€¾\\_\n    |   (smooth descent to minimum)\n    |\n    | Too Small Î±\n    | /â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾...\n    | (very slow, gets stuck)\n    |_________________________\n```\n\n**Solutions to Learning Rate Problems:**\n\n1. **Learning Rate Scheduling:**\n   - Start high, decay over time\n   - Examples: Step decay, exponential decay, cosine annealing\n\n2. **Adaptive Learning Rates:**\n   - **Adam**: Adapts per-parameter learning rates\n   - **RMSprop**: Uses moving average of squared gradients\n   - **AdaGrad**: Accumulates historical gradients\n\n3. **Learning Rate Warmup:**\n   - Start very small, gradually increase\n   - Prevents early instability\n\n4. **Grid/Random Search:**\n   - Try multiple learning rates: [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n   - Find optimal empirically\n\n**Best Practices:**\n- Start with Î± = 0.001 (1e-3) as baseline\n- Monitor training loss - should decrease smoothly\n- If loss spikes/diverges â†’ reduce learning rate\n- If loss decreases too slowly â†’ increase learning rate\n- Use adaptive optimizers (Adam) for automatic tuning\n\n**Key Insight:**\nLearning rate is one of the most important hyperparameters. Too conservative wastes time; too aggressive fails to converge. Modern deep learning relies heavily on adaptive optimizers to handle this automatically."
                }
            ],
            "deep_learning_architectures": [
                {
                    "instruction": "Explain the transformer architecture and why it revolutionized NLP.",
                    "input": "",
                    "output": "**Transformer Architecture (Vaswani et al., 2017):**\n\n**Revolution Summary:**\nTransformers replaced recurrent networks (RNNs/LSTMs) by using **attention mechanisms** exclusively, enabling:\n- Parallelization (vs. sequential RNNs)\n- Long-range dependencies capture\n- Scalability to massive datasets\n\n**Core Components:**\n\n**1. Self-Attention Mechanism:**\n```\nAttention(Q, K, V) = softmax(QK^T / âˆšd_k) V\n\nwhere:\n- Q (Query): What am I looking for?\n- K (Key): What do I contain?\n- V (Value): What do I actually convey?\n- d_k: Dimension of keys (for scaling)\n```\n\n**How Self-Attention Works:**\n- Each word attends to every other word in sequence\n- Learns which words are relevant to which\n- Produces context-aware representations\n- Example: \"The animal didn't cross the street because it was too tired\"\n  - \"it\" attends strongly to \"animal\" (not \"street\")\n\n**2. Multi-Head Attention:**\n- Run multiple attention mechanisms in parallel\n- Each \"head\" learns different relationships\n- Head 1: syntax, Head 2: semantics, Head 3: coreference, etc.\n- Concatenate outputs and project\n\n**3. Positional Encoding:**\n- Since there's no recurrence, need to inject position information\n- Uses sine/cosine functions of different frequencies\n- Allows model to understand word order\n\n**4. Feed-Forward Networks:**\n- Applied to each position independently\n- Two linear transformations with ReLU\n- FFN(x) = ReLU(xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚\n\n**5. Residual Connections + Layer Norm:**\n- Add input to output of each sublayer\n- Normalize to stabilize training\n- Enables training very deep networks\n\n**Architecture Stack:**\n```\nEncoder:\nâ”œâ”€â”€ Input Embedding + Positional Encoding\nâ”œâ”€â”€ N Ã— Encoder Layer:\nâ”‚   â”œâ”€â”€ Multi-Head Self-Attention\nâ”‚   â”œâ”€â”€ Add & Norm\nâ”‚   â”œâ”€â”€ Feed-Forward Network\nâ”‚   â””â”€â”€ Add & Norm\nâ””â”€â”€ Output to Decoder\n\nDecoder:\nâ”œâ”€â”€ Output Embedding + Positional Encoding\nâ”œâ”€â”€ N Ã— Decoder Layer:\nâ”‚   â”œâ”€â”€ Masked Multi-Head Self-Attention\nâ”‚   â”œâ”€â”€ Add & Norm\nâ”‚   â”œâ”€â”€ Multi-Head Cross-Attention (to encoder)\nâ”‚   â”œâ”€â”€ Add & Norm\nâ”‚   â”œâ”€â”€ Feed-Forward Network\nâ”‚   â””â”€â”€ Add & Norm\nâ””â”€â”€ Linear + Softmax â†’ Predictions\n```\n\n**Why It Revolutionized NLP:**\n\n1. **Parallelization:**\n   - RNNs process sequentially: wordâ‚ â†’ wordâ‚‚ â†’ wordâ‚ƒ\n   - Transformers process all words simultaneously\n   - Result: 10-100Ã— faster training\n\n2. **Long-Range Dependencies:**\n   - RNNs struggle with dependencies >100 tokens apart\n   - Attention directly connects any two positions\n   - O(1) path length vs. O(n) for RNNs\n\n3. **Scalability:**\n   - Enabled training on massive datasets (TB of text)\n   - GPT-3: 175B parameters\n   - Emergent abilities at scale\n\n4. **Transfer Learning:**\n   - Pre-train on massive unlabeled data\n   - Fine-tune on specific tasks\n   - Foundation for BERT, GPT, T5, etc.\n\n5. **Interpretability:**\n   - Attention weights show what model focuses on\n   - Visualize learned relationships\n\n**Impact:**\n- **2017-2018:** Replaces RNNs for machine translation\n- **2018:** BERT (bidirectional) dominates NLU tasks\n- **2019:** GPT-2 shows scaling properties\n- **2020:** GPT-3 demonstrates few-shot learning\n- **2022:** ChatGPT reaches mainstream\n- **2024:** Powers virtually all modern LLMs\n\n**Beyond NLP:**\n- Vision Transformers (ViT) for images\n- Audio processing\n- Protein folding (AlphaFold)\n- Reinforcement learning\n\n**Key Innovation:**\n\"Attention is all you need\" - removing recurrence and using pure attention unlocked massive parallelization and scalability, creating the foundation for modern AI."
                }
            ],
            "prompt_engineering": [
                {
                    "instruction": "Demonstrate advanced prompt engineering techniques.",
                    "input": "How can I improve this prompt: 'Write code for sorting'?",
                    "output": "**Prompt Engineering Analysis & Optimization:**\n\n**Original Prompt Issues:**\nâŒ \"Write code for sorting\"\n- Too vague (what sorting algorithm?)\n- No context (language, data type, constraints)\n- No success criteria\n- No format specification\n\n**Advanced Techniques to Apply:**\n\n**1. Be Specific (Add Context):**\n```\nðŸ”§ Improved:\n\"Write Python code to implement quicksort for sorting a list of integers.\"\n```\n\n**2. Specify Constraints & Requirements:**\n```\nðŸ”§ Better:\n\"Write Python code to implement quicksort with the following requirements:\n- Input: List of integers\n- Handle duplicates correctly\n- In-place sorting (O(log n) space)\n- Include docstrings\n- Add type hints\"\n```\n\n**3. Provide Examples (Few-Shot Learning):**\n```\nðŸ”§ Even Better:\n\"Write Python code to implement quicksort. Here are examples of expected behavior:\n\nInput: [3, 1, 4, 1, 5, 9, 2, 6]\nOutput: [1, 1, 2, 3, 4, 5, 6, 9]\n\nInput: []\nOutput: []\n\nInput: [1]\nOutput: [1]\n\nRequirements:\n- Handle edge cases (empty, single element)\n- In-place sorting\n- Include comprehensive docstrings\n- Add type hints\n- Time complexity: O(n log n) average case\"\n```\n\n**4. Chain-of-Thought Prompting:**\n```\nðŸ”§ Advanced:\n\"Implement quicksort in Python. Let's approach this step-by-step:\n\n1. First, explain the quicksort algorithm conceptually\n2. Identify the base case and recursive case\n3. Determine the partitioning strategy\n4. Implement the partition function\n5. Implement the main quicksort function\n6. Add error handling and edge cases\n7. Write example usage and test cases\n\nProvide well-documented code with explanations.\"\n```\n\n**5. Role-Based Prompting:**\n```\nðŸ”§ Expert-Level:\n\"You are a senior software engineer conducting a code review. Implement a production-ready quicksort in Python that would pass your review.\n\nRequirements:\n- Clean, readable code following PEP 8\n- Comprehensive docstrings (Google style)\n- Type hints for all functions\n- Edge case handling\n- Example usage demonstration\n- Time/space complexity analysis in comments\n\nProvide the implementation as you would write it for a critical production system.\"\n```\n\n**6. Output Format Specification:**\n```\nðŸ”§ Most Effective:\n\"Implement quicksort in Python with the following structure:\n\n```python\nfrom typing import List\n\ndef partition(arr: List[int], low: int, high: int) -> int:\n    \"\"\"\n    [Your implementation with docstring]\n    \"\"\"\n    pass\n\ndef quicksort(arr: List[int], low: int = 0, high: int = None) -> None:\n    \"\"\"\n    [Your implementation with docstring]\n    \"\"\"\n    pass\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_data = [3, 1, 4, 1, 5, 9, 2, 6]\n    quicksort(test_data)\n    print(test_data)  # [1, 1, 2, 3, 4, 5, 6, 9]\n```\n\nInclude:\n1. Full implementation with type hints\n2. Docstrings explaining parameters, returns, time/space complexity\n3. In-line comments for non-obvious logic\n4. Test cases covering edge cases\n\"\n```\n\n**Comparison Summary:**\n\n| Technique | Clarity | Completeness | Quality |\n|-----------|---------|--------------|----------|\n| Original | â­ | â­ | â­ |\n| + Context | â­â­ | â­â­ | â­â­ |\n| + Constraints | â­â­â­ | â­â­â­ | â­â­â­ |\n| + Examples | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |\n| + Chain-of-Thought | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |\n| + Role + Format | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |\n\n**Key Prompt Engineering Principles:**\n\n1. **Clarity > Brevity**: Be specific even if longer\n2. **Examples > Descriptions**: Show don't just tell\n3. **Constraints Matter**: Specify what you want AND don't want\n4. **Format Specification**: Define expected output structure\n5. **Context is King**: Background information improves relevance\n6. **Iterative Refinement**: Test and improve prompts\n\n**Meta-Lesson:**\nPrompt engineering is about reducing ambiguity and providing sufficient context for the model to infer your exact intent. The more specific and structured your prompt, the better the output."
                }
            ]
        }

        examples = []
        per_category = num_examples // len(self.categories)

        for category in self.categories:
            if category in templates:
                category_templates = templates[category]
                for i in range(per_category):
                    template = random.choice(category_templates)
                    example = TrainingExample(
                        instruction=template["instruction"],
                        input=template["input"],
                        output=template["output"],
                        domain=self.domain_name,
                        category=category,
                        difficulty=random.choice(["hard", "expert"]),
                        metadata={"generated_at": datetime.now().isoformat()}
                    )
                    examples.append(example)

        self.examples = examples
        logger.info(f"âœ… Generated {len(examples)} AI/ML examples")
        return examples


class AdvancedSoftwareDatasetGeneratorFull(AdvancedSoftwareDatasetGenerator):
    """Full advanced software dataset generator"""

    def generate_examples(self, num_examples: int) -> list[TrainingExample]:
        logger.info(f"ðŸ’» Generating {num_examples} advanced software examples...")

        templates = {
            "system_design": [
                {
                    "instruction": "Design a distributed URL shortener system like bit.ly.",
                    "input": "Handle 1000 writes/second and 10,000 reads/second. Ensure high availability and low latency.",
                    "output": "**URL Shortener System Design:**\n\n**Requirements Analysis:**\n- **Functional:**\n  - Generate short URLs from long URLs\n  - Redirect short URLs to original URLs\n  - Custom aliases (optional)\n  - Analytics (click tracking)\n\n- **Non-Functional:**\n  - 1000 writes/sec, 10,000 reads/sec (10:1 read-heavy)\n  - Low latency (<100ms)\n  - High availability (99.9%)\n  - Scalable\n\n**Capacity Estimation:**\n```\nWrites: 1000/sec\n= 86.4M/day\n= 31.5B/year\n= 315B over 10 years\n\nStorage per URL: ~500 bytes (URL + metadata)\nTotal storage: 315B Ã— 500B â‰ˆ 157.5 TB\n\nReads: 10,000/sec = 864M/day\n```\n\n**High-Level Architecture:**\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Clients â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n     â”‚\n     â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Load Balancerâ”‚ (NGINX/AWS ELB)\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n       â”‚\n       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â†“                     â†“                  â†“\n â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n â”‚ API      â”‚          â”‚ API      â”‚      â”‚ API      â”‚\n â”‚ Servers  â”‚          â”‚ Servers  â”‚      â”‚ Servers  â”‚\n â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n       â”‚                     â”‚                  â”‚\n       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n       â”‚                                        â”‚\n       â†“                                        â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Cache Layer  â”‚                         â”‚ Database â”‚\nâ”‚  (Redis)     â”‚                         â”‚ Cluster  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n       â”‚\n       â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Analytics DB â”‚\nâ”‚ (ClickHouse) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Key Design Decisions:**\n\n**1. URL Encoding Strategy:**\n\nOption A: Hash-based (MD5/SHA256)\n- Pros: Fast, deterministic\n- Cons: Collision handling needed, not shortest\n\nOption B: Base62 Encoding of Auto-increment ID âœ…\n```python\ndef encode_base62(num: int) -> str:\n    chars = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    if num == 0:\n        return chars[0]\n    result = []\n    while num:\n        result.append(chars[num % 62])\n        num //= 62\n    return ''.join(reversed(result))\n\n# Example:\n# ID 1 â†’ '1'\n# ID 62 â†’ '10'\n# ID 1000000 â†’ '4c92'\n```\n\n**Benefits:**\n- 6 characters = 62^6 = 56.8 billion combinations\n- 7 characters = 62^7 = 3.5 trillion combinations\n- No collisions (unique IDs)\n- Short URLs\n\n**2. Database Design:**\n\n**Schema:**\n```sql\nCREATE TABLE urls (\n    id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    short_code VARCHAR(10) UNIQUE NOT NULL,\n    long_url VARCHAR(2048) NOT NULL,\n    user_id BIGINT,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    expires_at TIMESTAMP NULL,\n    INDEX idx_short_code (short_code),\n    INDEX idx_user_id (user_id)\n);\n\nCREATE TABLE clicks (\n    id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    short_code VARCHAR(10),\n    clicked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    user_agent VARCHAR(255),\n    ip_address VARCHAR(45),\n    referer VARCHAR(255),\n    INDEX idx_short_code (short_code),\n    INDEX idx_clicked_at (clicked_at)\n);\n```\n\n**3. Caching Strategy:**\n\n```\nRedis Cache (TTL: 24 hours)\nâ”œâ”€â”€ Key: short_code\nâ”œâ”€â”€ Value: {long_url, user_id, created_at}\nâ””â”€â”€ Eviction: LRU\n\nCache Hit Rate: ~90% (read-heavy workload)\n```\n\n**Read Flow:**\n```\n1. Check Redis cache\n   â”œâ”€â”€ Hit â†’ Return long_url (1-5ms)\n   â””â”€â”€ Miss â†’ Query database â†’ Update cache (20-50ms)\n2. Redirect client (HTTP 301/302)\n3. Async: Log click to analytics DB\n```\n\n**4. Scalability:**\n\n**Database Sharding:**\n```\nShard key: hash(short_code) % num_shards\n\nFor 1000 writes/sec:\n- Start with 4 shards (250 writes/sec each)\n- Vertical partitioning: separate clicks table\n```\n\n**ID Generation:**\nUse distributed ID generator:\n- **Snowflake algorithm** (Twitter's approach)\n- **Database auto-increment with ranges** (Flickr's approach)\n- **UUID** (not ideal - too long)\n\n**5. Availability & Reliability:**\n\n- **Database**: Master-slave replication + failover\n- **Cache**: Redis Cluster with replication\n- **Load Balancer**: Active-passive pair\n- **Multi-region deployment** for geo-distribution\n\n**6. API Design:**\n\n```python\n# Create short URL\nPOST /api/shorten\nBody: {\"long_url\": \"https://example.com/very/long/url\"}\nResponse: {\"short_url\": \"https://short.ly/abc123\"}\n\n# Redirect\nGET /{short_code}\nResponse: HTTP 301 Redirect to long_url\n\n# Analytics\nGET /api/analytics/{short_code}\nResponse: {\"clicks\": 1523, \"countries\": {...}}\n```\n\n**7. Optimizations:**\n\n- **CDN** for static content\n- **Connection pooling** for databases\n- **Bloom filter** for non-existent URLs (avoid DB queries)\n- **Rate limiting** to prevent abuse\n- **Async analytics** to not slow down redirects\n\n**8. Monitoring:**\n\n- Latency (p50, p95, p99)\n- Cache hit rate\n- Database query time\n- Error rates\n- Traffic patterns\n\n**Estimated Costs (AWS):**\n```\n- EC2 (10 Ã— t3.medium): $300/month\n- RDS (db.m5.xlarge): $400/month\n- ElastiCache (cache.m5.large): $150/month\n- ALB: $50/month\n- Data transfer: $100/month\n\nTotal: ~$1000/month for 1M requests/day\n```\n\n**Conclusion:**\nThis design handles the required scale with room for growth, optimizes for the read-heavy workload through aggressive caching, and ensures high availability through redundancy and replication."
                }
            ],
            "algorithms_data_structures": [
                {
                    "instruction": "Explain when and why to use different data structures.",
                    "input": "Array vs Linked List vs Hash Table vs Binary Search Tree - when should I use each?",
                    "output": "**Data Structure Selection Guide:**\n\n**1. ARRAY**\n\n**When to Use:**\n- Need fast random access by index (O(1))\n- Know size in advance (or can estimate)\n- Sequential iteration is common\n- Memory locality matters (cache-friendly)\n\n**Use Cases:**\n- Fixed-size collections\n- Lookup tables\n- Matrix operations\n- Image pixel data\n- Time-series data\n\n**Operations:**\n- Access: O(1)\n- Search: O(n) unsorted, O(log n) sorted\n- Insert/Delete: O(n) (shifting required)\n- Space: O(n)\n\n**Example:**\n```python\n# Fast random access needed\nscores = [95, 87, 92, 78, 88]\nprint(scores[2])  # O(1) access\n```\n\n---\n\n**2. LINKED LIST**\n\n**When to Use:**\n- Frequent insertions/deletions at beginning\n- Don't know size in advance\n- Don't need random access\n- Implementing stacks/queues\n\n**Use Cases:**\n- Undo functionality (doubly-linked)\n- Music playlist (next/previous)\n- Browser history\n- Memory management (free lists)\n\n**Operations:**\n- Access: O(n)\n- Search: O(n)\n- Insert/Delete at head: O(1)\n- Insert/Delete at arbitrary position: O(n)\n- Space: O(n) + pointer overhead\n\n**Example:**\n```python\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n# Efficient insertion at front\ndef prepend(head, data):\n    new_node = Node(data)\n    new_node.next = head\n    return new_node  # O(1)\n```\n\n---\n\n**3. HASH TABLE (Dictionary/HashMap)**\n\n**When to Use:**\n- Need O(1) average lookups\n- Key-value association needed\n- Checking existence (membership testing)\n- Counting occurrences\n\n**Use Cases:**\n- Caching\n- Database indexing\n- Frequency counting\n- Removing duplicates\n- Implementing sets\n\n**Operations:**\n- Access: O(1) average, O(n) worst case\n- Insert: O(1) average\n- Delete: O(1) average\n- Space: O(n)\n\n**Example:**\n```python\n# Fast lookups and counting\nword_count = {}\nfor word in text:\n    word_count[word] = word_count.get(word, 0) + 1\n\n# O(1) lookup\nif 'python' in word_count:\n    print(word_count['python'])\n```\n\n---\n\n**4. BINARY SEARCH TREE (BST)**\n\n**When to Use:**\n- Need data sorted\n- Range queries needed\n- Need both O(log n) search AND maintain order\n- Predecessor/successor queries\n\n**Use Cases:**\n- Maintaining sorted streaming data\n- Range searches (find all between X and Y)\n- Auto-complete systems\n- Priority queues (with heap variant)\n\n**Operations (balanced BST):**\n- Access: O(log n)\n- Search: O(log n)\n- Insert: O(log n)\n- Delete: O(log n)\n- Traversal in sorted order: O(n)\n- Space: O(n)\n\n**Example:**\n```python\nclass TreeNode:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef search_bst(root, target):\n    if not root or root.val == target:\n        return root\n    if target < root.val:\n        return search_bst(root.left, target)\n    return search_bst(root.right, target)\n\n# O(log n) search in balanced tree\n```\n\n---\n\n**DECISION FLOWCHART:**\n\n```\nStart\n  |\n  â†“\n[Need O(1) lookups by key?]\n  â”œâ”€Yesâ†’ HASH TABLE\n  â””â”€Noâ†’ [Need sorted order?]\n          â”œâ”€Yesâ†’ [Frequent inserts/deletes?]\n          â”‚       â”œâ”€Yesâ†’ BINARY SEARCH TREE\n          â”‚       â””â”€Noâ†’ SORTED ARRAY + Binary Search\n          â””â”€Noâ†’ [Frequent inserts/deletes?]\n                  â”œâ”€Yes (at front)â†’ LINKED LIST\n                  â””â”€Noâ†’ [Random access needed?]\n                          â”œâ”€Yesâ†’ ARRAY\n                          â””â”€Noâ†’ LINKED LIST\n```\n\n**COMPARISON TABLE:**\n\n| Operation | Array | Linked List | Hash Table | BST (balanced) |\n|-----------|-------|-------------|------------|----------------|\n| Access by index | O(1) | O(n) | N/A | N/A |\n| Search | O(n) | O(n) | O(1) avg | O(log n) |\n| Insert (start) | O(n) | O(1) | O(1) avg | O(log n) |\n| Insert (end) | O(1)* | O(n) | O(1) avg | O(log n) |\n| Delete | O(n) | O(1)** | O(1) avg | O(log n) |\n| Sorted traversal | O(n log n) | O(n log n) | N/A | O(n) |\n| Space overhead | Low | High | Medium | Medium |\n| Cache-friendly | Yes | No | No | No |\n\n*Amortized for dynamic arrays\n**If you have pointer to node\n\n**REAL-WORLD EXAMPLES:**\n\n```python\n# Use Array: Simple list of items\nitems = [1, 2, 3, 4, 5]\n\n# Use Hash Table: Fast lookups\nuser_cache = {user_id: user_data}\n\n# Use BST: Sorted data with updates\nfrom sortedcontainers import SortedDict\nleaderboard = SortedDict()  # Maintains score order\n\n# Use Linked List: Undo/Redo\nclass UndoManager:\n    def __init__(self):\n        self.history = LinkedList()  # Easy insert/delete\n```\n\n**KEY TAKEAWAY:**\n- **Array**: Known size, random access, cache-friendly\n- **Hash Table**: Key-value, O(1) lookups, no order\n- **BST**: Sorted order, O(log n) operations, range queries\n- **Linked List**: Dynamic size, frequent insertions at ends\n\nChoose based on your access patterns and performance requirements!"
                }
            ]
        }

        examples = []
        per_category = num_examples // len(self.categories)

        for category in self.categories:
            if category in templates:
                category_templates = templates[category]
                for i in range(per_category):
                    template = random.choice(category_templates)
                    example = TrainingExample(
                        instruction=template["instruction"],
                        input=template["input"],
                        output=template["output"],
                        domain=self.domain_name,
                        category=category,
                        difficulty=random.choice(["hard", "expert"]),
                        metadata={"generated_at": datetime.now().isoformat()}
                    )
                    examples.append(example)

        self.examples = examples
        logger.info(f"âœ… Generated {len(examples)} advanced software examples")
        return examples


# Export updated generators
__all__ = [
    'AIMLDatasetGeneratorFull',
    'AdvancedSoftwareDatasetGeneratorFull'
]
